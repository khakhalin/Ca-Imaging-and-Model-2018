# Ca img paper
Graph analysis of a collision detection network in the tectum, and its replication in a simple computational model

## Abstract

Looming detection is 

We still don’t know actual internal organization of networks in the optic tectum, but we have some idea of which developmental rules govern the maturation of these networks. For this study, we first built a simplified developmental model of the tectum, and allowed it to evolve under the influence of four shaping factors: STDP, synaptic competition, intrinsic plasticity, and structured sensory inputs that represent geometrically realistic maneuvering with occasional collisions. We observed that under these conditions the model network rapidly evolved structured connectivity, as well as selectivity for looming stimuli in isolated cells. We then conducted a series of Ca imaging experiments in live awake *Xenopus* tadpoles presented with several types of visual stimulation. We estimated looming stimulus selectivity in individual cells, and reconstructed connections between tectal cells based on their activity. This allowed us to compare the predictions drawn from the model to patterns found in physiological data, recorded at two different developmental stages. We describe several network properties of local connectivity in the tectum, and changes in these properties during network maturation. We found that most, although not all, model predictions were confirmed in our experiments. 


# Introduction

Few sensory stimuli are as conspicuous and ill boding as a visual looming stimulus. A retinal projection that is small, but quickly grows in size, may promise either a painful collision, or a meeting with a hungry predator, and so it inherently calls for an action, such as an avoidance maneuver, freezing, defensive posturing, or a blink. Moreover, looming detection has to be fast to be meaningful. Not surprisingly, it is described in virtually every type of animals that uses vision, including [many links].

In different animals, looming detection seems to rely on a variety of networks and solutions (Sun Frost), including integration of dimming detectors in the retina [mice ref?], opponent motion detection [fly], and competitive spike-frequency adaptation [locust]. Even within an single clade of anuran amphibians (frogs), animals seem to employ several competing looming detection mechanisms, such as non-linear detection of retinal oscillations [Baranauskas], and rebound of recurrent activity [Jang]. Moreover, it seems that at least some of these competing mechanisms may lead to different types of avoidance responses, as it was described in insects [Card], fish [many], and tadpoles [Khakhalin 2014].

While this co-existence of multiple alternative solutions for looming detection may seem overwhelming, it actually matches the recent insight from the field of machine learning. Simple, crude ways of detecting important features of sensory stimuli are critical for training more sophisticated and efficient networks, that are then used for more nuanced analysis of the sensory world at later stages of development [Marblestone 2016]. In case of looming stimuli in tadpoles, early in development, when collision detection is weak [Dong], animals may use “hardwired” dimming receptors in the retina to detect collisions, but also to “bootstrap” more sophisticated motion-dependent networks in the tectum. Later, these motion-dependent networks can serve as a first line of defense, identifying early phases of looming, and providing information to motor neurons in the hindbrain to perform course correction [Khakhalin 2014], yet dimming detectors could remain as a backup, and mediate a more urgent and less coordinated response. Moreover, every time collision avoidance is not performed perfectly, these sensorimotor networks can be further refined, relying on inputs from the lateral line and mechanosensory detectors in the skin.

Traditionally, the golden standard for a mechanistic explanation was to demonstrate that activation of a certain structure is sufficient and required for a behavior [ref]. This reductionist approach works well in some systems, such acoustic startle detectors [m-cell refs], or central pattern generators in the spinal cord [refs], that are compact and isolated enough. Yet most systems in the brain act as complex, interconnected, distributed dynamical systems, which makes it hard to represent them as a sum of “parts”, with distinct functions ascribed to each of these parts [Gao 2015]. It is possible to study these complex systems statistically, identifying properties that hold “on average”, and differ in functional and dysfunctional networks, but a statistical approach typically does not grant true insight into the “meaning” of these properties [Bassett 2017]. For example, knowing that a disordered brain does not adhere to the same statistics as a normal brain would not necessarily tell us how to “fix” this brain, even if we could rewire individual neurons in a targeted fashion.

A more promising approach is to study the origin of functional structures in the brain. Most neural networks are not truly hardwired, but dynamically “evolve” from a set of developmental rules, similar to how it happens for Turing patterns or fractals, and may also rely on streams of structured sensory information coming from the external world [Gao 2015]. This insight is encouraging, as the agents that follow these developmental rules are individual cells and their compartments, such as active dendrites and synapses, which implies that while the ultimate product of these rules may be exceedingly complex, the rules themselves have to be relatively simple [Bassett 2018]. It may be therefore, that our best chance to truly “understand” the brain lies in studying the sets of rules that lead to the development of functional networks [Linderman 2017]. This approach proved to be extremely fruitful recently, as several complex network phenomena, including visual receptive fields [ref], grid cells [ref], and decision circtuis [refs; Engert], were shown to evolve spontaneously in systems governed by intuitive developmental rules and clear behavioral goals. 

In this paper, we look at the structural correlates of looming selectivity in the developing optic tectum of Xenopus tadpoles. Tadpole tectum offers a unique opportunity to study the dynamics of sensory integration, as its neurons are excessively plastic [Pratt; Busch], are strongly connected to each other [James, refs], and develop highly reliable looming selectivity within about a week, as tadpoles mature from developmental stage 46 to stage 49 [Dong, Khakhalin]. The refinement of tectal connectivity is dominated by spike-time-dependent plasticity [Poo, more], which is known to favor the development of synfire chains [refs]: ensembles of neurons that are activated in a sequence [ref], and selectively respond to certain patterns of temporal activation [Clopath 2010]. Because tadpole tectal neurons spike relatively slowly, with broad spikes and long refractory periods [Ciarleglio; Jang], these synfire chains are expected have delays of about 10 ms from one neuron to another [Ciar], compared to 2 ms in the mammalian cortex [ref]. This slower signal propagation brings intratectal connections to the threshold of direct detectability by fast Ca imaging techniques that operate at rates of ~100 frames/s, allowing us to observe not just co-firing of neurons within an ensemble or clique [refs including correlation papers and recent cliques, Avitan 17], but a propagation of signal through these ensembles.

We used high-speed Ca imaging data to infer connectivity patterns in small sub-networks within the tectum, described their in younger and older tadpoles, and compared them to connectivity patterns predicted by a simulation model. We show that our model successfully predicts several aspects of topology and functionality in looming-sensitive networks. From this, we conclude that the architecture of tectal networks may be explained by an interplay of simple developmental rules with structured visual information coming from the eye. Finally, we gradually deconstruct our model, and show which developmental rules are most critical for tectal network development.

# Results

For all mathematical methods used in this paper, in the main text we provide their names, and the interpretation of the results, while definitions and details are given in the “Methods” section. In all statistical analyses, we report p-values without correction, and interpret them according to Fisher, and not Neyman-Pearson philosophy [Greenland 2016]. This approach is preferable for this study, as many of our analyses are not independent, but are also not redundant, and rely on different metrics and H0 hypotheses. At the interpretation step, we pay more attention to hypotheses supported by several alternative analyses, pointing in the same general direction.

We performed Ca imaging recordings from 14 stage 45-46 tadpoles, and 16 stage 48-49 tadpoles, recording responses from 128±40 tectal cells (between 84 and 229). Here and below, “±” after the mean denotes standard deviation, and unless stated otherwise, these sample sizes (n=14 and 16 animals for stage 46 and 49 tadpoles respectively) apply to all analyses between younger and older animals. To each tadpole, we presented a sequence of three different stimuli, always in the same order: a looming stimulus, followed by a full-field flash, followed by a spatially “scrambled” looming stimulus. Scrambled stimuli were identical to looming, except that the visual field was split into a 7x7 grid of square tiles, and these tiles were randomly rearranged in space. In total we presented 60±11 stimuli to every animal (60 or more stimuli in 28/30 cases), which means that a stimulus of every type was presented at least 20 times. High speed calcium imaging recordings were performed from one layer of “deep” principal tectal neurons in the tectum [Xu; Truszkowski]; from these videos we then extracted fluorescence traces, and inferred average spiking of each neuron within every frame.


## Responses and stimulus selectivity

TODO *Typical shape of responses across all brains.*

The **total spiking** **output** of observed tectal networks tended to be higher in response to looming stimuli than to flashes (39±29% higher for younger, and 25±25% higher for older tadpoles). There was no change in overall preference for looming stimuli in development (pt=0.15). The total tectum output was variable from one experiment to another, and while the majority of preparations showed a significant selectivity for looming stimuli (p_t<0.05 in 11/14 and 9/16 experiments for stage 46 and 49 tadpoles respectively), the remaining 10 experiments showed no significant preference for looming stimuli. Note that the majority of experiments with no full-brain preference for looming stimuli were from older, and so presumably more developed animals.

The difference between responses to looming and scrambled stimuli was even more diverse: some preparations did not have a preference between the stimuli (9/14 and 7/16 for younger and older tadpoles respectively), some preferred looming stimuli to scrambled (1/14 and 4/16), while some significantly preferred scrambled stimuli to looming (4/14 and 5/16). This individualized preference for either looming or scrambled stimuli was most likely an artifact of our experimental protocol, as “scrambled” stimuli were uniquely rearranged for each experiment, but were kept constant within each experimental session. It seems that this design produced less salient stimuli in some experiments, but more salient stimuli in others. These results also support our earlier reports that in tadpoles, the total tectal respond more strongly depends on the dynamics of visual stimuli, rather than on its geometry [Khakhalin 2014, Jang 2015].

To quantify **selectivity of individual tectal cells**, we used Cohen’s effect sizes of their responses to stimuli (the difference in mean responses, divided by the pooled standard deviation of response amplitudes). We calculated two different measures of selectivity: that for looming stimuli over full-field flashes (a type of selectivity that can rely on both stimulus dynamics and its spatial organization), and looming over scrambled stimuli (that can only rely on spatial organization, as both stimuli shared the dynamics). On average, tectal cells showed selectivity for looming stimuli, with no change from younger to older tadpoles (0.67±0.50 and 0.46±0.47 respectively, pt=0.3). The share of cells that responded to looming stimuli stronger than to flashes also did not change in development (84±23%, 77±21%; pt=0.4). There was however a change in the distribution of cell selectivities within individual networks: while the variance of selectivity values within each brain did not change (pt=0.3), there was a change in skew (pt=0.02), which was positive in younger animals, but negative in older ones. The skew decreased because the difference between the top-selective (90th percentile) and median selective cells within each brain became lower in older animals: 0.75±0.26 in younger, but 0.53±0.27 in older animals (pt=0.03). Both of these results are unexpected: stage 49 tadpoles perform better than stage 46 tadpoles in collision avoidance tests [Dong], and so we expected them to have a higher selectivity to looming stimuli, and also to have a more well-defined subset of looming-selective cells, as described in adult frogs, and other vertebrates [XXX]. Yet in our experiments overall selectivity for looming stimuli did not change, and a subpopulation of strongly selective cells became less prominent in older animals.

We then considered the second, more computationally demanding definition of selectivity: a preference for spatially organized looming stimuli over scrambled stimuli. On average, tectal cells did not have a preference between these two stimulus types (average selectivity of -0.07±0.33 in younger tadpoles, -0.04±0.49 in older ones), and the value of average selectivity also did not change in development (pt=0.9). The share of cells that responded to looming stimuli stronger than to scrambled stimuli was at a chance level for both developmental stages (46±31%, 48±37%, pt=0.9). Similarly, there was no change in either within-brain variance of this selectivity (pt=0.9), or the 90-50 percentile asymmetry of values (pt=0.8). 

The selectivity for scrambled stimuli over flashes correlated with selectivity for looming stimuli over flashes in both developmental groups: within-brain r=0.82±0.13, p1t=3e-12 for younger animals, and 0.75±0.18, pt1=3e-11 older ones; no change in development (pt=0.3). On the contrary, the preference for looming over flashes did not correlate with preference for looming over scrambled stimuli (r=0.03±0.29, p1t=0.7 for stage 46; 0.13±0.30, p1t=0.1 for stage 49). This further demonstrates that during collisions, the majority of cells in the tectum responded to stimulus dynamics, rather than to its geometry.

To assess the cell-to-cell variability of responses, we performed **exploratory factor analysis** (principal component analysis, followed by varimax rotation) of cell responses within each brain. We restricted this analysis to responses to one stimulus type at a time, as otherwise factor analysis detected cell-to-cell differences in stimulus selectivity, while we wanted to look at the differences in dynamics within a family of responses. For looming stimuli, the 1st and 2nd principal components explained on average 19%±7% and 4%±1% of variance in younger tadpoles, and 24%±14% and 3%±1% in older tadpoles, and different cell response profiles, as detected by factor analysis, primarily differed in response latency (**Fig**). Early responding cells tended to group together in one part of the visualized field (**Fig**), and we were able to find a center of this cell cluster for each experiment, by maximizing the correlation coefficient between the physical distance to cluster center, and the relative amplitude of short-latency activation (XXX-XXX ms) of tectal cells. The resulting best fit correlation was significant in 13/14 and 16/16 experiments, for stages 46 and 49 respectively, while if we reshuffled cell identities, the best fit correlation was significant only in 6% of cases, as expected by chance. We interpreted this as a direct observation of a retinotopic map in the tectum [Ruthazer 2004]. Curiously, while visual projections to the tectum are known to be actively remodeled in development [REFS], there was no difference in the precision of the functional retinotopic map between younger and older tadpoles, as best fit correlation coefficients did not increase in development (r=0.43±0.25 and 0.35±0.21 respectively, p_t=0.38), and remained relatively low, as it was described for zebrafish [Avitan 2016].

Knowing where our looming stimulus is projected, we could then check whether looming-selective cells were more likely to be found in the center of the expanding activation area (as it would be expected if collision detection was local and retinotopic [Sun Frost?], or if it was primarily based on feedback excitation [Jang Khakhalin]), or at the periphery (as predicted by the synfire model [refs]). We found that selectivity for looming stimulus tended to decrease with distance from the estimated projection center (**Fig**) for both stage 45 (individual correlations significant p_r<0.05 in 7/14 individual experiments, average r=−0.15±0.31), and stage 49 tadpoles (p_r<0.05 in 12/16, average r=-0.10±0.35; no change between developmental stages p_t=0.7). Similarly, in both younger and older tadpoles, selectivity decreased with response latency during a looming response (stage 46: p_r<0.05 in 11/14 animals, average r=-0.32±0.12; stage 49: p_r<0.05 in 8/16 animals, average r=-0.12±0.24). On average, the correlation between response latency and cell selectivity was weaker in older tadpoles (p_t=0.01). To sum up, selectivity to looming stimuli was stronger in cells with shorter latencies (~20 ms), located in the center of the emerging spatial tectal response, and this effect tended to decrease in development.

Finally, as a most holistic way to quantify overall tectal network selectivity, we looked at the **total predictive power** of tectal responses [Avitan 2016]. To do so, we ran a logistic regression on one half of data, linking the total response of each recorded tectal cell in each trial to the visual stimulus type used in this trial. Then we measured the quality of this linkage on the second half of recorded data. The quality of prediction was rather low: 59%±12% for younger, and 62%±13% for older tadpoles, with no change in development (p_t=0.6).

## Variability and ensembles

We then assessed the trial-to trial variability of tectal responses, to see whether it changed in development, as it was previously reported for variability of spontaneous activity [Xu 2011; Avitan 2017]. We used the principal component analysis of response waveforms, and looked at the total number of components that was needed to describe 80% of variability in the data [Avitan 2017]. We found that this number was similar in young and old tadpoles, but showed a minor increase in response richness in older animals (insignificant for each stimulus alone, but consistent across stimuli): 51±14 and 65±28, pt=0.1 for responses to looming stimuli; 49±12 and 62±32, pt=0.2 for flashes; 51±14 and 64±26, pt=0.1 for scrambled stimuli. 

To better describe the structure of network activation during sensory responses, we used spectral clustering technique to identify ensembles of tectal cells that tended to be either active or not active together on a trial-by-trial basis. Unlike for recordings of spontaneous activity, we could not easily aggregate activity states into clusters [Avitan 2017], as the states of our networks were driven by shared and repeated sensory inputs. Instead, we subtracted normalized average responses of each cell from its responses in individual trials, and calculated pairwise correlations of the remaining “anomalies” of trial-by-trial activation for every cell (**Fig**, see “Methods”). We turned these pairwise correlations into pairwise distances in a multidimensional space, and ran a series of spectral clustering partitions [Ng Jordan Weiss 2002], assigning cells to different clusters. Finally, of all possible partitions, we picked a partition that maximized spectral modularity on a weighted graph of pairwise cell-by-cell correlations [Gomez 2009; Newman 2006]. We found that the number of ensembles was not significantly different between younger and older tadpoles (10±5 in stage 45, 11±11 in stage 49; pt=0.9; **Fig**), but in older tadpoles ensembles were slightly less isolated from each other, producing lower values of network modularity (0.14±0.05 to 0.09±0.06, pt=0.03; **Fig**). This implies stronger coordination between the activity of ensembles in older tadpoles.

The tectal ensembles revealed by this method tended to be spatially localized within the brain, rather than distributed across the network (**Fig**): and cells that shared an ensemble were on average closer to each other than to cells from different ensembles, for both younger (29±9% closer) and older tadpoles (25±10% closer; no difference in development pt=0.3).

## Network reconstruction

The high speed of video acquisition used in this study (83 frames/s) allowed us to look not just at synchronous correlations between the activity of individual neurons, but at the propagation of spiking signal through the network. In Xenopus preparations, it takes about 5 ms for a presynaptic action potential to elicit release in a typical tectal synapse [Khakhalin 2012; Ciar 2015], and about 10-20 ms for a model synaptic current to depolarize postsynaptic neuron above the spiking threshold [Ciar 2015; Busch 2017], which makes the frame-to-frame delay in our video recordings (12 ms) ideally posed to infer neuronal connections from activity.

To reconstruct network connectivity, we used the transfer entropy approach [Gourevitch 2007; Stetter 2012, more refs]. Intuitively, for each pair of neurons i and j, we quantified the amount of additional information that the past activity of neuron i can provide to predict the current activity of neuron j. This is somewhat similar to calculating a cross-correlation between the activity of neuron i and the activity of neuron j in every next frame, but unlike for a correlation, transfer entropy approach does not make assumptions about the nature of the influence neuron i has over neuron j. It means that in a general case, TE has a higher power to detect causation from activity [Stetter 2012, more refs].

In our experiments, all tectal neurons received shared inputs from the eye, which recruited them in a similar sequence in each trial. This complicated connectivity inference, as neurons could spike in a sequence both because they were connected, and because they received innervation from sequentially activated areas of the retina. To compensate for this, we randomly reshuffled trials for every neuron, calculated average transfer entropy on reshuffled data, and subtracted this value from the transfer entropy on trial-matched data [Gourevitch 2007, some Wollstadt] (see Methods for details). In essence, instead of analyzing raw responses, we analyzed small deviations from the average response, and quantified whether these deviations tended to propagate through the network, from one neuron to another. The reshuffling step also allowed us to calculate a p-value for each pair of neurons, which quantified how extreme the observed value of transfer entropy for this pair of neurons was, compared to a value that would arise from shared inputs, but no causal connections. 

We interpreted the table of transfer entropy values as an approximation of a weighted adjacency matrix W that described connections between tectal neurons, with element $$w_{ji}$$ describing the strength of connection from neuron i to neuron j. We calculated W and corresponding p-values independently on looming, flash, and scrambled stimuli, and then used these independent estimations to ensure some level of internal replication within each experiment (see Methods). 1.6±1.4% of all edges were replicated between stimuli, which was significantly higher than 0.6±0.09% as expected if edge discoveries were random (paired t-test p_t=9e-9). Finally, in each experiment, we introduced cut-offs on $$w_{ji}$$ and p-values, to eliminate weak and noisy edges from the connectivity graph. We automatically adjusted these cut-offs in every experiment, ensuring that the average node degree for each network (the average number of connections per node) is close to 1.0. It means that in each network, the total number of edges in the graph was equal to the number of nodes (recorded cells); an approach frequently used in noisy network analysis [Stetter 2012, more refs]. With the average total degree set at 1.0, the effective significance thresholds on p-values were between 0.001 and 0.007 (median 0.004). 

The simplest statistical property of a probabilistic networks is its degree distribution: the share of nodes with different number if incoming (k_in) and outgoing (k_out) connections. We compared degree distributions between networks detected in younger and older tadpoles (**Fig. X**), and found that more mature networks contained fewer unconnected cells (k_in=0, p_t<0.03) and fewer cells with high number of connections (k_in=5, k_out=6, p_t=0.01 in both cases), but more cells with intermediate number of connections (k_in=2, p_t=0.001, and k_out=2, p_t=0.04). If we approximated degree distributions (excluding k=0) with a power law, as would be true for a scale free network (**Fig. X**, see Methods), the distribution constant \minus \gamma was smaller in younger (1.48±0.19) than in older tadpoles (1.82±0.25, p_t=2e-4). The distribution of degrees became sharper, with a steeper drop between the occurrence rate of weakly connected and highly connected cells. **Fig. X** shows an exaggerated illustration of what this difference in degree distributions mean for the network topology: developed tectal networks are closer to **Fig. Xb**, with “egalitarian”, linear chains of connected neurons (degree k=1) and forks (k=2), while younger neurons have more hyperconnected hubs (k>5) and unconnected nodes (k_in=0), similar to **Fig. Xa**. 

An unusual feature of our calcium imaging protocol, compared to most common calcium imaging techiques, is that the signal-to-noise ratio varied greatly from one cell to another, depending on how far its cell body was from the focal plane, and how much dye it absorbed during staining, which in turn depended on the share of its surface exposed to the chamber solution. As a result, the share of cells with weak signal, that appeared unconnected due to low values of pairwise transfer entropy, might have been affected by the physical curvature of the brain. While we performed all dissections to minimize the curvature, and maximize the intersection between the set of stained cells and the focal plain, arguably there could be uncontrolled differences in the quality of this overlap between younger and older tadpoles. Therefore, for all network analyses, presented below, we wanted to minimize the effect of poorly resolved cells on network parameter estimations, and restricted our analysis to the largest weakly connected component of the network. There was no difference in the number of weakly connected components detected in younger and older tadpoles (50±14 and 50±26 for stages 45 and 49 respectively, N=14 and 16; p_t=0.9), but in older animals the largest weakly connected component included a higher share of cells (50±6% and 64±12% respectively; p_t=4e-4), which matches the higher share of unconnected cells (k=0) described above.

Synaptic connections in the tectum exhibit robust spike-time dependent plasticity (STDP) [Poo papers], and a known effect of maturation in networks dominated by STDP is that with time neuronal connections become highly asymmetric. Indeed, if cells i and j are reciprocally connected, every time j spikes after i, weight $$w_{ij}$$ will be increased by STDP, while weight $$w_{ji}$$ will be diminished [Abbott and other refs]. We found that in our data the share of bidirectional edges ($$w_{ij}, w_{ji}>0$$) among all detected edges (0.3$$\pm$$0.3%) was smaller than what would be expected for random edge rearrangement in graphs of our size (0.4$$\pm$$0.1%, paired p_t=0.02), indicating asymmetric information flow in the tectum. Moreover, the share of bidirectional edges decreased in development, from 0.4$$\pm$$0.3% in younger animals to 0.2$$\pm$$0.2% in older animals (p_t=0.03), suggesting that STDP is shaping emerging network topology at these developmental stages.

We then looked at whether connected cells were more likely to be located closer to each other in the tectum. We found that the average distance between connected cells was indeed shorter than the average distance we would get on a randomized graph: 18$$\pm$$10% shorter for stage 45, and 17$$\pm$$8% shorter for stage 49 tadpoles (individually significant with p_t<0.05 for 13/14 and 16/16 experiments respectively). Contrary to our expectations, and in contrast to the visual inputs to the tectum [Tao Poo 2005; refs], the intra-tectal connectivity did not become more compact in development (pt=0.7). This may mean that tectal network relies on far-reaching recurrent connections to integrate visual information across the visual field, as it was suggested by our previous computational study [Jang]. 

## Network properties

We then measured several standard network properties for every connectivity graph reconstructed from tadpole tecta. For each age group, we used two different approaches to check whether the values of these network properties were statistically unusual. First, we compared values actually observed in the network to values obtained on a set of randomized graphs, in which the distribution of edge weights w_ij, and the number of edges adjacent to each node (unweighted in- and out- degrees) were kept fixed (a procedure known as degree-preserving rewiring [XXX]). Second, we repeated this calculation, but this time also randomized node degrees, by reassigning existing edges among all possible edges in a graph (simple rewiring). Finally, we compared network statistics for younger and older tadpoles to each other, to see whether any network properties changed in development. In theory, the use of two different comparisons with randomized graphs would allow us to explain any change in network properties by a combination of two different effects: the change in node degree distribution, and a change in the large-scale structure of the graph [Angsman]. The summary of all comparisons is given in **Table**.

The measure of large-scale connectivity in the graph, named weighted network **efficiency**, is defined as the average inverse shortest path, for any two nodes in the network [Rubinov 2010, Latora Marchiori 2001]. This value is high when, on average, there is a short path between any two randomly chosen nodes, and so signals can be easily propagate within the graph; the value is low when some nodes are located far from each other on a graph (**Fig. example)**. Traditionally, this measure interprets graph edges as pairwise distances between the nodes, with high value edges representing weak connections; for a graph of weights however, high value edges represent strong connections, so network efficiency is calculated on inverse weights R_ij = 1/w_ij [Rubinov 2010]. In our experiments, network efficiency (0.004±0.002 for stage 46, 0.002±0.002 for stage 49 tadpoles) was slightly lower than expected for a random network with matching degree distribution (d=-0.3, paired p_t=0.04 and d=-0.3, paired p_t=0.06 for younger and older tadpoles respectively), or a fully randomized network with the same distribution of edge weights (d=-0.5 and -0.05; paired p_t=0.02 and 0.01 for younger and older tadpoles respectively). Efficiency did not seem to change in development (d=-0.8, p_t=0.06).

Network **clustering** coefficient describes the small-scale heterogeneity in the network [Fagiolo 2007], and is defined as the relative frequency of two neighboring nodes forming a triangle via a third node that is connected to both of them (**Fig. example**). The value of clustering coefficient in our networks was small (2.4±2.5 e-3 for stage 46, 1.5±1.6 e-3 for stage 49 animals), but was still slightly larger than expected in a rewired network with same degree distribution (d=0.5 and 0.6, paired p_t=0.01 and 0.02 for younger and older animals), and similar to what would be expected for a random graph (d=0.1 and 0.2; paired p_t=0.4 and 0.3). This suggests that the observed distribution of degrees made the network less clustered than a random network, but the assortative arrangement of nodes countered this effect. There was no change in clustering in development (d=-0.4, p_t=0.3, **Fig.**).
 
The network **modularity** is the most commonly used measure of large-scale network heterogeneity [Leicht 2008, Newman]. A network with high modularity can be split into a set of subnetworks that are connected within themselves stronger than expected on average, while the connections between these subnetworks are weaker than average for the graph (**Fig. example**). In our experiments, networks in both younger and older animals were not significantly less ore more modular than expected for a randomized network with the same degree distribution (d=0.2 and 0.3, paired p_t=0.2 and 0.06 respectively). Randomized networks with matching degrees, however, were significantly less modular than a fully randomized network (d=-0.6 and -0.4, paired p_t = 3e-7 and 5e-5 respectively), suggesting that observed functional connectivity networks were less modular than a random network, but this difference in modularity was fully explained by the distributions of node degrees. The network modularity also increased in development (d=1.0, p_t = 0.01), and this change was mediated by a difference in edge weights distribution, as the effect did not disappear if networks were randomly rewired before comparison (for permutatoin rewiring, d=1.2, p_t=0.003).

The property of **hierarchical flow** is a rather complicated measure [XXX] based on the distribution of Katz centrality values within the network (see Methods for the definition) [XXX]. Intuitively, the flow hierarchy is high when connections between different subsets of nodes largely point in the same direction, as it happens in layers networks, networks with chains of directed edges, or activity sinks [Mones Vicsek Vicsek 2012]. We hypothesized that a network with dedicated looming detectors may exhibit non-trivial flow hierarchy, but the data we observed was hard to interpret, as structural non-randomness and the non-randomness of degree distribution affected hierarchical flow coefficients in opposite ways, with these two effects cancelling each other. Indeed, observed networks were more hierarchical than randomized networks with matching degrees distribution (d=1.5 and 1.1, paired p_t=1e-04 and 1e-03 for younger and older tadpoles respectively), but randomized networks with matching degrees distribution were substantially less hierarchical than fully randomized networks with matching distribution of w_ji (d=-4.5 and -2.6, paired p_t = 4e-8 and 2e-6 respectively). There was no difference in development (d=-0.3, p_t=0.4).

We also introduced a measure of **cyclicity**, which quantified the presence and average strength of various directed cycles starting from a node and returning back to the same node, relative to a similar value on a full graph without self-loops, with the same number of nodes (see Methods). The cyclicity of observed graphs (1.4±1.4 e-4 for younger, 3.8±4.3 e-5 for older tadpoles) was higher than expected for this degree distribution (d=0.8 and 0.8, paired p_t=0.002 and 0.01 for younger and older animals respectively), but the value for a random graph with a matching degree distribution was lower than for a fully randomized graph (d=-2.2 and -2.6, paired p_t=3e-5 and 2e-6 respectively). This non-trivial pattern means that compared to a fully random graph, the distribution of degrees did not favor short cycles (as it had fewer nodes with d>2), yet actually observed graphs were structurally enriched with short cycles. There was no change in cyclicity in development (d=-0.1, p_t=0.9).


## Selectivity Mechanisms

Regardless of the exact architecture of the collision-detecting networks in the tectum, it is safe to assume that looming selective neurons integrate multiple streams of information coming from different parts of the visual field. We therefore hypothesized that looming-selective neurons may be non-randomly located within the detected connectivity graph. For example, they could have received more incoming connections, on average, compared to non-selective cells. To get a glimpse of possible looming selectivity mechanisms, for each reconstructed network we calculated correlation coefficients between looming selectivity of each neuron and several values that quantified its location within the graph; then looked whether these correlation coefficients were significantly different from zero across experiments. 

For the number of incoming connections, we found that in stage 46 tadpoles, selectivity did not correlate with weighted in-degree (r=0.08±0.24, p_t1=0.2; individual r>0 in 9/14 experiments), but in stage 49 tadpoles the correlation was significant, even if small (r=0.07±0.13, p_t1=0.03; individual r>0 in 12/16 experiments), indicating that in older tadpoles selective cells received slightly more incoming connections, compared to non-selective cells.

As a slightly more advanced way to quantify information sinks, we calculated the **Katz centrality** measure for each of the neurons [refs]. Katz centrality is similar to pagerank centrality, but better represents signal propagation in a neural network, and also better generalizes to weighted graphs [refs]. Nodes with high Katz centrality have many paths leading to them, so a spike originating at random within a graph is more likely to eventually excite a cell with a high value of Katz centrality, compared to a cell with a low value. Similar to the result for in-degree, we found that in younger tadpoles, there was no correlation, on average, between Katz centrality and cell selectivity (r=0.08±0.25, p_t1=0.25, individual r>0 in 8/14 experiments), while in older tadpoles Katz centrality correlated with selectivity (r=0.08±0.13, p_tq=0.03; individual r>0 in 12/16 experiments).

As cells with higher Katz centrality tend to be activated more often, we checked whether cell selectivity for looming stimuli correlated with its average activity during the recording. We found that for both younger and older animals, actively spiking cells tended to have stronger looming selectivity (r=0.20±0.24 and 0.14±0.19 respectively, in both cases r is significantly >0, p_t1=0.01 and 0.01). There was no change in this correlation over development (p_t=0.4).

If looming-selective cells are gathering information from the network, it was also plausible to hypothesize that they would be more likely to be connected to each other, rather than to non-selective cells. To test whether it is true, we used the weighted **assortativity of selectivity** values: essentially, a correlation between the selectivity scores of every pair of cells connected by an edge, with a strength of this edge weighted into the calculation. Similar to the tests described above, the selectivity values of connected cells did not correlate in graphs reconstructed from younger tadpoles (r=0.05±0.12, p_t1=0.14, individual r>0 in 9/14 experiments), but they did correlate in graphs recorded in older tadpoles (r=0.05±0.07, p_tq=0.01; individual r>0 in 11/16 experiments). Together, these results suggest that the distribution of selective cells in developed network becomes less random, which may mean that distributed network calculations start to play a bigger role in looming stimulus detection in older animals.

We also hypothesized that on average selectivity for looming stimuli may gradually “accumulate” as the signal propagates through the network, and so “downstream” cells on the receiving end of a strong edge within the graph would, on average, be more selective than “upstream” cells. This hypothesis proved to be wrong in both younger and older tadpoles: across all experiments, 52±7% of strong edges (top quartile of w_ji values) led from cells with less to more selective cells, which is not different from chance rate (p_t1=0.1 for analysis across experiments, n=30). A weighted average increase in selectivity between two cells connected by an edge was 0.02±0.07 (p_t1=0.2, n=30), and there was no change in this value in development (p_t=0.2, n=14, 16).

# Model

As tectal networks we studied are complex, distributed, and noisy, our experimental results defied easy mechanistic interpretation, especially as in some cases we used several alternative ways to quantify an intuition, and got conflicted results. For example, for statistical network properties (**Fig**), the effects of non-random degree distribution and large-scale graph structure often affected network properties in opposite directions, leading to small total effects. Similarly, when we tried to quantify the “integrative” position of each cell within the connectivity graph, only some of these measurements correlated with cell selectivity for looming stimuli, and it was not clear which of these measurements are objectively “better”, in terms of encapsulating the true properties of the network. Moreover, the sample sizes we have to work with are relatively small (14 and 16 observed networks for two developmental stages, respectively), and and only allowed us to detect large changes in network properties (d \approx 1.0, assuming p_t<0.05 and 80% power), which further complicated the interpretation of results.

To compensate for these limitations, and to provide a theoretical counterpoint to our observations, we created a mathematical model of the developing tectum, and ran this model through the same set of measurements that we applied to our experiments. The model consisted of 81 artificial neurons, arranged in a 9 by 9 grid, that were originally all connected to each other (every neuron to every neuron) with random positive (excitatory) synaptic weights (**Fig**). The model operated in 10 ms increments, and we interpreted the output of each neuron as its expected spiking frequency. At each time frame we looked at the activity of the network in the previous frame, calculated the synaptic inputs each neuron would send to each other neuron, and summed them up to calculate the total activation of each neuron. We then used a sliding logistic function to estimate the probability of spiking in postsynaptic neurons, in response to this total activation. See “Methods” for the formulas, and the detailed description of the model.

To allow the network develop in time, we introduced three simple developmental rules: spike-time-dependent plasticity (STDP), homeostatic plasticity, and synaptic competition. Our implementation of STDP approximated biological STDP, as observed in the tadpole tectum [refs]: if two cells were active in two consecutive 10 ms time frames, and they were connected with a synapse, the weight of this synapse was increased. Conversely, if two cells were active within the same time frame, the weight of a synapse connecting them was decreased, as it means that one cell would try to activate another cell after it already spiked. The homeostatic plasticity adjusted the excitability threshold of each neuron, trying to keep its spiking output constant on average [refs]. The synaptic competition attempted to keep the total strength of synaptic inputs to each neuron, as well as the total strength of synaptic outputs from each neuron, close to constant, by scaling all synapses of both input and output neurons down every time one synapse increased in efficiency [refs].

With these three rules at play, we exposed the model to patterned sensory stimulation, modeling retinotopic inputs from the eye. For the main series of computational experiments, we hypothesized that in biological networks, STDP-driven changes may be amplified by a learning signal [refs on latent learning?], coming either from dimming receptors in the retina [refs], or mechanosensory systems in the hindbrain [refs]. Therefore, in this set of experiments we only exposed our model to looming stimuli (but see below for sensitivity analysis). The network was allowed to develop for 12500 time steps (500 slightly variable looming stimuli), and we saved its topology at five equally spaced time points during this process, from a naive network, to its final state. We ran independent simulations 50 times, and for each network snapshot we analyzed the properties of the connectivity graph, as well as the responses of each network to “model visual stimuli” that included a looming stimulus, a scrambled stimulus, and a full-field flash. We then analyzed these data in the same way as we did for biological experiments.

The summary of modeling results is shown in **Fig.**, and their comparison to imaging experiments, is given in **Table 2**. In development, the network became selective for looming stimuli, both in terms of the total response (by the end of developme tresponding to looming 99±9% stronger than to flash) and mean selectivity (mean Cohen d of responses = 1.09±0.10). The share of cells selective for looming stimuli also increased, and then saturated at ~98% level, as did the selectivity of the top 10% of most selective cells. 

To see whether in our model the nature of visual stimuli could be reconstructed from network activation, we used a logistic regression on a vector of total cell responses, similar to how it was done for biological data. We then used a different dataset, consisting of equal shares of looming and non-looming stimuli, to assess the accuracy of our model. The prediction power increased in development, and plateaued at the accuracy level of ~95%. This suggests that a retinotopic STDP-driven network can achieve highly reliable looming detection if it is equipped with an output layer, potentially representing motor neurons in the hindbrain, and the weights of connections to this output element are also modified through reinforcement learning.

Unlike in biological experiments, the model was selective for looming stimuli over scrambled stimuli at the full network level, and by the end of the training period, responses to looming stimulus were 43±7% stronger than to scrambled. Mean selectivity of individual cells, defined as a Cohen d for responses to looming compared to scramble, was 0.48±0.07, and 84% of cells were selective for looming stimuli, which is also different from what we observed in biological experiments. The selectivity for looming over flash correlated with selectivity for scrambled stimuli over flash on a cell by cell basis (r=0.17±0.10).

The positioning of selective cells within the network was different in model, compared to biological experiments. While in tadpoles, selective cells tended to be located in the middle of the retinotopic field, in the model they tended to be located on the periphery, and selectivity for looming stimuli positively correlated with the distance from the network center. Similar to tadpoles, however, this correlation disappeared in development (from r ~ 0.75 in a naive network, to r~0.25 in a trained network). Despite the fact that the edges of looming stimuli generally traveled from the center of the retinotopic field to the periphery, there was no correlation between the weight of a cell-to-cell connection and its being facing outward from the network center (r=8e-5), which means that connections were equally likely to face outward and inward. Similarly to biological networks, selective cells tended to be closer to each other (29±3% closer) than expected by chance, yet unlike what we observed in biological networks, this locality of connections was refined in developments.

We then looked at topological and functional correlates of looming selectivity in model networks. Selective cells tended to be more spiky (r=0.34±0.12), with very mild increase in development. They tended not to be a part of a cluster (final correlation with local clustering coefficient r=-0.26±0.12). Unlike in biological networks, in the model selective sells were not special in terms of their Katz centrality (r=0.02±0.13), and they did not tend to receive an unusual number of incoming connections (r=0.00±0.12). Selective cells tended to be connected to each other (weighted assortativity of 0.24±0.07), and on average selectivity did not increase over edges (r=0.00±0.14). In naive networks ~85% of strong edges (top 50% by weight value) tended to lead from less selective sells to more selective ones, but in developed networks this share was reduced to chance value (51±0.03).

The variability of responses to looming stimuli over time, quantified as the number of principal components required to describe 80% of response variability, mildly increased in development, from 28±1 to 37±1. The number of ensembles detected in the network did not change in development, and stayed around 8 to 8.5±0.5, but the share of variance in network responses explained by the involvement of different ensembles, increased from ~35% for a naive network, to ~50% by the end of learning. As in biological results, cells that formed an ensemble were about 10% closer to each other than random cells in the network, and, in developed networks, were 2.2±0.5 times more likely to be connected to each other.

The distribution of degrees in the model was very similar to that in the biological data set: the share of weakly connected cells (weighted in degree < 0.5) plummeted from ~50% in naive networks to 9±2% by the end of training. On the contrary, the share of cells with degrees of 1 and 2 increased from ~40% to 83±2%.  With this changes, the power constant for the degree distribution changed from \gamma ~-1.5 to -1.96±0.03 for both in- and out-degrees, which also qualitatively matched changes in biological experiments.

Finally, we observed that most network measures changed with network development: efficiency and modularity increased, while clustering and cyclicity decreased. In all three cases, the changes were mainly due to changes in weight and degree distribution, as they persisted if calculated on networks randomized with degree-preserving rewiring (Fig). The hierarchical flow increased mildly in development, and this was entirely due to structured changes in network topology, as the effect was not present in rewired graphs.

To assess whether the predictions of the model were confirmed in experiments, we formulated a list of “atomic”, elementary statements about each of the measures were analyzed, looking at whether they changed in development, whether they increased or decreased, and whether they differed from similar measures in a randomized network. For each of these statements, we then compared the model to the experimental data, and assessed each comparison as either a “match”, a “disagreement”, or an unclear territory (Table 2). The main reason for the third, “unclear” option is that in the model, several network properties rapidly changed early in development, but then reached a plateau, or even reversed the direction of the change. In this light, when assessing whether the model replicated the phenomena observed in real animals, we have first to make an important assumption about whether stage 46 tadpoles corresponds to a mid-point of network development, or whether both stage 46 and stage 49 tadpoles represent relatively “developed” networks that are kept dynamically fine-tuned. The decrease in the number of reciprocal connections between younger and older tadpoles seems to suggest that at stage 46 STDP is still actively eliminating connections from the network, making it as a relatively early developmental stage. On the other hand, the absence of improvement in stimulus identity prediction from the neuronal activity, from younger to older tadpoles, seem to suggest that both stages would fall on the “plateau”, at least as far as the looming selectivity is concerned. In view of this uncertainty, and honoring the fact that the sample sizes of our imaging experiments were rather low, if the model predicted a change, but this change was not observed in the experiment, we did not count it as an explicit contradiction.

## Sensitivity analysis

This is all nice, but is a model of OT in particular even necessary, or would a simple chaotic STDP-driven network a-la those old Abbot papers (ref) be enough? To test that, let’s look at a network that developed under a drive from random uncorrelated stimuli. (Comparison. vis and shuffle may be presented as intermediate stages of “randomization”)

How sensitive is the change in network parameters to other assumptions we made in our model? Here are some sensitivity analyses…

Note that the “global limitation on the number of edges” has an additional benefit of matching our choice of threshold for connections in experimental data, so going with this type of negative regulation means that our modeling data may be more easily comparable to our experimental data. It is not too physiologically plausible though, therefore more comparisons below.

# Discussion

**For why scramble was significantly stronger than looming in some experiments, but weaker in others**. Because spatial rearrangement was unique for each experiment (different across experiments, but constant within each experiment), it is possible that some of our rearrangements produced an objectively “worse” stimuli (as we expected), but some produced a supernormal stimulus, or a sort of a visual illusion that looked more salient than a collision. A good way to test this hypothesis in the future would be to rearrange the stimulus in several different ways within each experiment, and see whether the preference for either looming or scrambled stimulation at the level of tectal network as a whole is indeed tadpole-dependent, or stimulus-dependent. Regardless of the answer to this question, however, we can safely state that at the very least scrambled looming stimuli are about as salient for stage 46-49 Xenopus tadpoles as looming stimuli. It also suggests that the neural representation of the collision trajectory, and the calculation of the most effective escape route that was observed in our prior behavioral experiments [Khakhalin 2014], are more likely to happen at the interface between the tectum and the hindbrain motor nuclei, or even in these nuclei, rather than in the tectum.

It is known that that cells in younger Xenopus tadpoles have on average lower excitability [Ciarleglio, other refs], and so are expected to have a lower signal to noise level. As our connectivity analysis relied on equalizing all graphs to the same average degree of 1.0, this could have affected our graph estimation by making it closer to a random graph. While this problem is unavoidable, one can also argue that lower excitability, and thus lower levels of causality in spiking, are true properties of functional network connectivity at these developmental stages. The connectivity graphs we present in this paper ultimately reflect not only synaptic, but also intrinsic differences in network, to a degree in which neuronal excitability affected signal propagation in the tectum.

The non-linearity of signal integration in the tectum can be greatly enhanced by strong expression of NMDA receptors in the postsynaptic terminals of “integrator” cells, as it was recently shown for the case of multisensory integration in the Xenopus tadpole [ref]. We can therefore expect that the activity of collision selective cells, and especially those located at the junctions of synfire chains, would be strongly affected if NMDA transmission is downregulated in the middle of an imaging experiment; likely more so than the cells in the middle of synfire chains.

As we wanted to compare developed tectal networks to undeveloped ones, we picked stage 45 tadpoles as a representation of an “undeveloped brain”, which was of course a choice dictated by necessity. We reasoned that at stage 45-46 tadpoles don’t perform collision avoidance as robustly as older, stage 49 tadpoles [Dong], yet at stages 45-46 tadpoles are large enough to make the required surgery possible, unlike at stages 42-43. Also, at stages 42-43 the tecta are much smaller, in terms of the number of cells, which would have made a direct comparison of these recordings problematic. It is quite possible though that by the age 45 the tectal circuits involved in collision detection are mostly developed, and it is the motor circuit in the hindbrain that is lagging in development. If this is true, this would explain why we observed no different in tectal selectivity between stage 45 and 49 tadpoles.

The weakest aspect of the experimental half of this study is variable quality of staining with calcium sensitive dye. As staining procedure involved a detergent, and called for high concentrations of dye, the most successful preparations were those that received the highest possible concentration of dye that did not yet kill the cells. Many preparations however either fell short of this target, and had a higher than optimal level of noise, or got overexposed, and did not respond to stimulation. The variable amount of noise between experiments might have affected the distribution of weights in the reconstructed adjacency matrix, even for those weights that were significantly different from 0. Two aspects of our study design however partially compensate for this limitation. For one, at the data acquisition stage, we alternated between experiments in younger and older tadpoles, so any any unintentional variations in the protocol that happened over time would be equally affecting both experimental groups. Second, in the analysis of network properties, we paid most attention to differences between observed and rewired graphs, and these pairs of graphs had the same distribution of weight edges, and so should not be strongly affected by differences in these distributions.

A common estimation is that at developmental stage 49, Xenopus tadpoles have about 10-15 thousand cells in each of the tecta (based in imaging studies [refs], tectal network has about 40 cells across, packed 6-10 cells deep in its thickest part, and tapering towards the edges). On the other hand, in our study we reconstructed connectivity within the top layer of cells, in a field of about 12 by 12 neurons, which suggests that our reconstructions covered about 1% of the full tectal network. With a coverage so sparse, we could hope to only detect the strongest and most robust changes in network organization, and were bound to severely underestimate its effects. For example, sampling 10% of nodes in a large graph allows the discovery of only 1% of connections within this graph, 0.1% of all 3-node cycles, 0.01% of all 4-node cycles etc. Because of that, even a drastic increase in the number of cycles within a full graph would hardly be detected in its random cross-sections. This is further complicated by the fact that younger tadpoles have fewer neurons in their brains, and so in them the subset of cells that we recorded consistuted a larger share of total neurons in the tectum. Because of that, strictly speaking, all our statements about large-scale network properties, such as connectivity or modularity, were made not about the full network of the optic tectum, but about the “local subnetwork of randomly observed 100 neurons” within it.

HOWEVER the graphs we reconstructed differed from random graphs in several important, and interpretable ways, which indirectly supports the reality of the effects we describe.

Mention the STDP controversy: does it even exist? If it happens to work nicely in our model, does it provide some sort of evidence that at the very least it may be useful for development?

On ensembles: our way of data acquisition is not ideal for ensemble detection, as we have a strong shared input that drives network activity and most probably differentially activates different functional ensembles in the network. During spontaneous activity, each subnetwork, such as the collision detection subnetwork, would be activated relatively independently from the rest of the tectum, and then the activity would propagate within this subnetwork (as it is more tightly connected), enabling ensemble detection. We on the other hand reliably stimulate this network in every third stimulus. Incidentally it also means that it would be less likely to be activated during other stimuli due to short-term homeostatic mechanisms, such as Na channes inactivation, short-term synaptic depression, and maybe even intrinsic plasticity. Our only hope to detect ensembles *within* each subnetwork is to get a failure of transmission, or a particularly noisy input, which is arguably a more rare event, especially at the time frame we are working with (0.1 ms, as opposed to ~2s in Avitan 2017).

In the modeling part of our study, we looked at the development of a looming-detecting network that only was rewarded, and thus rewired, after direct frontal collisions, but a real swimming animal experiences different types of looming stimuli that come from different directions, and have different degrees of a translational component. The final result of an unsuccessful collision avoidance would also be somewhat different for a real animal: the rapid dimming of optic receptors would be experienced by different parts of the retina; different subsets of the lateral line and mechanosensory receptors would get activated. Therefore, a full brain may harbor several similar overlapping subnetwork, each selective for stimuli of different geometry, getting reinforced by different learning signals, and projecting to different subsets of the motor network.

# Methods
## Statistics and reporting

Unless stated otherwise, all values are reported as mean ± standard deviation. For most common tests, the type of a test is indicated by the subscript for its reported p-value: p_t for a two-sample t-test with two tails and unequal variances; p_t1 for a one-sample two-tail t-test; p_r for a Pearson correlation test; p_binom for a one-tail binomial test, and p_F for the analysis of variance (ANOVA, ANCOVA, multilinear regression).

Note also that in this paper we consistely describe adjacency matrices as they are used in computational neuroscience, where w_{ji} is a weight of an edge coming from node i to node j, which is different from how adjacency matrices are usually used in graph theory, where A_{ji} would typically mean an edge from node j to node i.

## Experiments

Our experiments followed the procedure previously described in [Xu 2011] and [Truszkowski 2017], with visual stimulation described in [Khakhalin 2014]. All procedures were in accordance with Brown University IACUC protocols. Briefly, tadpoles were kept in Steinberg’s solution, on a 12/12 light cycle, at XXX or XXX C for XXX-XXX days, until they reached Nieuwkoop-Faber developmental stages 45-49. In each experiment, we anesthetized a tadpole with XXX% tricaine solution for 5 minutes, then paralyzed it by immersion in XXX solution of XXX for XXX minutes, and pin it down to a Sylgard block within the recording chamber. We exposed the optic tectum, and cleaned the membrane on one side of the tectum [refs]. Tadpoles were pinned at a slight angle of XXX-XXX, with one side turned slightly up, to keep the tectal surface flat for imaging. The chamber was then filled with artificial cerebro-spinal fluid solution (ACSF: XXX FORMULA XXX), just enough to keep the tadpole completely covered. We surrounded the tadpole with a small circular enclosure XXX in diameter, made of a thicker part of a standard plastic transfer pipette (XXX firm, product code), to achieve higher concentration of Ca-sensitive dye in the solution immediately surrounding the tadpole. We prepared 30 µl of Ca sensitive dye (XXX solution of XXX dye in a medium consisting of 4% F-127 detergent in 96% DMSO by weight), and agitated this solution in an ultrasound sonicator for 15 minutes, then added 30 µl of ACSF from the chamber to the vial, and sonicated for 10 minutes more. We then used a pipette to transfer this solution to the chamber, pouring it slowly right over the exposed tectum; placed the chamber in the dark (closed cabinet and under foil), and waited for 1 hour. Then the circular enclosure was removed; the preparation was washed with 10 ml of fresh ACSF 3 times; the chamber was filled with 10 ml of fresh ACSF, and transferred under the scope. 

Visual stimulation was provided with a previously described setup [Khakhalin 2014], consisting of a XXX x XXX LCD screen illuminated by a XXX LED, with the image projected on a XXX optic multifiber. We brought the other end of the optic fiber to the left eye of the tadpole, placing the fiber XXX um away from the lens, and on the axis of the eye, to have the image projected to the center of the retina. The stimulation sequence consisted of three stimuli: looming stimulus (in which a circle appeared in the center of the field, its radius growing linearly from 0 to full-field within 1 second), full-field flash, and spatially “scrambled” stimulus. For the scrambled stimulus, we divided the field of view into a grid of XXX by XXX squares and randomly reassigned these squares within the image. The result was a stimulus that was identical to looming stimulus in terms of its total brightness at every time step, and presented fragments of a moving edge locally (within every square in a reshuffling grid), but lacked large-scale spatial organization. The permutation of squares within the grid was randomized for each experiment, but consistent within all trials within an experiment. The stimuli were delivered every XXX seconds, in a sequence “looming, flash, scrambled”, for XXX minutes (XXX stimuli overall). The stimuli were created in Matlab (XXX), using Psychtoolbox (XXX); the stimulation was triggered by a signal from the imaging software (see below).

The right tectum was imaged using a XXX scope and a XXX camera, excitation provided by a XXX light source, and XXX filter (XXX wavelength). We used NIS elements software (XXX) to record the activity. The field of view was binned at 8x8, which resulted in a 130x130 image covering the field of view of XXX um. The data was acquired with 10 ms auto-exposure, which led to actual frame rate of 84 frames per second (11.9 ms per frame). For each preparation, we used a focal plane that produced images of as many cells as possible, having them also as sharp as possible, which usually meant a plane focused “in-between” the topmost and bottom-most cells within the field of view. To keep the signal to noise ratio consistent throughout the experiment despite the ongoing bleaching of the Ca sensor, we started with relatively weak illumination (with neutral density filter ND4 engaged) and no signal amplification by the camera (EM gain of 0). We then increased the EM gain level gradually after every 12 stimuli, to keep the signal level approximately constant. Once EM gain setting reached the value of “8”, we increased illumination strength by disengaging one of the density filters, reduced EM gain back to 0, and repeated the process. 

Videos were processed in XXX software offline, after the experiment was over. Circular regions of interest of equal size (XXX pixels for different) were manually positioned over neurons with well defined, highly variable Ca responses. The average fluorescence within each region of interest was quantified, exported, and loaded to Matlab. We processed fluorescence traces with a non-negative deconvolution algorithm [Vogelstein], and used its output without thresholding, interpreting it as an estimation of both timing and number of possible spikes produced by each cell [refs?]. We chose this approach, as depending on the overlap each cell body had with the focal plane, and the amount of dye sequestered, different neurons had very different signal-to-noise ratios, which complicated the matter of finding a single threshold. This decision also shaped all further steps of analysis, as in our dataset both poorly resolved cells with low spiking activity were represented not by spike traces that were mostly silent, but by traces that approached a maximum entropy, uniform distribution of estimator values. For the purposes of deconvolution, in each recording the reference cell was selected automatically, as the cell with 5th highest amplitude fluorescence response. 

In this series of experiments, we did not attempt to match inferred spike trains to the “ground-truth” electrophysiological recording from a reference cell, as overall validity of this calcium imaging protocol was justified previously [Xu, Torrey]. We also did not perform background subtraction [refs], as most effect of background fluorescence were expected to be cancelled out at later stages of analysis. The main risk of not subtracting the background is that unsubtracted traces may contain a superposition of axonal spiking and synaptic activation in the neuropil. In our experiments, the bulk of neuropil activation was expected to be similar in every trial, as stimuli presented to the tadpole were always the same. Moreover, our video acquisition was by design highly sensitive to fluorescence sources lying within the focal plane, which means that the neuropil signal was both greatly attenuated, and spatially averaged. As deconvolution operation is close to linear, and as we did not perform spike thresholding, any shared neuropil signal would be deconvolved, “hidden” in inferred spike-trains, and later cancelled out during trial-reshuffling, together with other average signals that did not change from one trial to another (see below). Finally, we did not address motion artifacts, as in our preparation they were synchronous in all cells (manifested as parallel displacement of signal sources from fixed ROIs), and therefore only introduced a fixed bias to all TE estimations.

## Analysis

**Basic analysis**
*How response amplitudes were measured (cumulative on a range XXX to XXX)*

*Selectivity: Cohen d*

To verify that Cohen d captured the nature of selectivity to a certain stimulus type, we also quantified selectivity as McFadden’s pseudo-$$r^2$$, by fitting total spiking response of each cell to stimulus identity, and assessing the quality of this fit (see Methods). Across all cells recorded in all experiments, both measures correlated (r=0.66), and the results on pseudo-$$r^2$$ were qualitatively similar to those on Cohen’s d. 

*How latency was calculated*

**Ensemble analysis**
To find ensembles of cells that tended to be co-active together, we used a modified spectral clustering procedure [Ng 2002] and the definitinon of spectral modularity [Newman 2006], generalized to weighted oriented graphs. First, for each stimulus type, for each cell i, and separately for each experimental trial k, we unbiased and normalized each activity response a^k_i(t), by subtracting its mean, and dividing the result over standard deviation:

a^k_i(t)’ = (a^k_i(t)-b^k_i)/\sigma^k_i}

where b^k_i = \sum_{t=1}^T{a^k_i(t)} and sigma^k_i = \fraq{1}{T-1}\sum_{t=1}^T{(a^k_i(t) - b^k_i))} .

Then, for each cell, we calculated the average response across all trials of the same type:  \over{a_i(t)} = \fraq{1}{n}\sum_{k=1}^n{a^k_i(t)}, and subtracted these average responses from each trial, which resulted in a vector of a trial-by-trial deviations from the average response:

a^k_i(t)’’ = a^k_i(t)’ - \over{a_i(t)}

We then used these vectors of deviations from the mean, concatenated across all trials, to calculate a cross-correlation matrix, to see which cells tended to be unusually active or unusually inactive together:

c_{ij} = \text{corr}(a’’_i(k,t),a’’_j(k,t))

We calculated adjusted correlations c_{ij} separately for each of three types of stimuli (flash, scramble, and looming), and averaged these three estimations c_{ij}^s, to arrive at a, hopefully, less noisy estimation of adjusted cross-correlation. We then removed negative correlations, replacing them with zeroes.

c’_{ij} = \text{max}(0, \frac{1}{3}\sum_{s}{c_{ij}^s}

We then roughly followed the spectral clustering approach by [Ng 2002], with some adjustments that seemed appropriate for ensemble detection.  We first transformed our correlation matrix c_{ij} into a matrix of pairwise Euclidean distances:

d_{ij} = 2(1-c_{ij}), 

and then to affinity matrix:

A_{ij} = exp(-d_{ij}/\sigma) 

where \sigma is a free parameter that we set at 10000. We then calculated a diagonal degree matrix D such that D_{ij} = 0 for i \neq j , and D_{ii} = \sum_k{A_ik} otherwise; used it to build a Laplacian matrix L, such as:

L_{ij} = A_{ij}/\sqrt{D_ii*D_jj}

, and found eigenvectors x_1 .. x_n of matrix L. Then we selected a number of ensembles to find k, going through all values from 1 (no ensembles) and up to the number of cells (each cell as a separate ensemble). For each k, we found first k largest eigenvectors of L, stacked them in columns, and renormalized each row of this matrix to give it unit length:

U_{lm} = x_{lm}/\sqrt{\sum_{z=1}^{k}{x_{lz}^2}}

where x_{lm} is an m-th element of l-th eigenvector of L. We then used k-means clustering on rows of U as points in R^k, looking for k clusters.

Once rows of U (and so cells in the original data) were assigned to k clusters, we calculated spectral modularity of this division on the original matrix w_{ij}, using a weighted directed modification of classic formula from [Newman 2006]:

Q_k = \frac{1}{4m}\sum_{ij}{\delta_{ij}(w_{ij}-\frac{d^{out}_i d^{in}_j}{2m}})

Here d^{out}_i and d^{in}_j are weighted out- and in-degrees for nodes i and j respectively: 
d^{out}_i = \sum_k{w_ik} , and d^{in}_j = \sum_k{w_kj} ; m is the total number of edges involved: m = \sum_{ij}{w_{ij}}/2 , and \delta_{ij} is a signal matrix with \delta_{ij}=1 for nodes i and j that belong to the same cluster, and \delta_{ij} = 0 otherwise. We then found the number of clusters K that, after spectral clustering, produced highest modularity Q_K across all Q_k, and used K as an estimation of the number of ensembles in the network, and corresponding cluster allocation - as the allocation of cells to these ensembles.

**Network reconstruction**
For network reconstruction, we used a modified Transfer Entropy (TE) calculation, adapted from [Stetter 2012, Gerhard 2013, Gourevitch 2007]. Mathematically, fast Ca imaging recordings, as used in this study, provides a middle ground between commonly used, slower Ca imaging data and multielectrode recordings. In most Ca imaging recordings, the frame acquisition time (~100 ms) is an order of magnitude longer than the transmission time between neurons (~ 2 ms), which biases analysis towards co-activation analysis. In our data, the high rate of acquisition (12 ms per frame) was very close to typical cell-to-cell activation transmission time in the tectum, so we restricted our analysis to interactions between the activity of each cell at a frame i and their activity at the next frame i+1, ignoring both longer (multiframe), and same-frame interactions.

For each cell, we took its inferred activity train, and binned it at 3 levels, classifying every frame as either a frame with high, medium, or low activity. For each cell, we used 1/3 and 2/3 quantiles of its inferred activity train values as levels thresholds, so that all three types of frames were equally frequent, as this optimized the entropy of representation, while retaining low binning count. Then for each pair of neurons i and j we calculated the probability p(k_j^1,k_j^0,k_i^0), which showed the conditional probability of neuron j being in state k_j^1 (1 to 3) at moment t, if this neuron was in a state k_j^0 at the previous frame t-1, while input neuron i was in state k_i^0 at frame t-1. From this set of probabilities, we also calculated conditional probabilities of P(k_j^1 \mid k_j^0), and finally calculated the total transfer entropy as

T_{ij} = \sum_{lmn=1}^3{P(k_j^1=l,k_j^0=m,k_i^0=n)}\cdot \log(\frac{P(k_j^1=l \mid k_j^0=m,k_i^0=n)}{P(k_j^1=l \mid k_j^0=m)})

$$T_{ij} = \sum_{lmn=1}^3{P(k_j^1=l,k_j^0=m,k_i^0=n)}\cdot \log(\frac{P(k_j^1=l \mid k_j^0=m,k_i^0=n)}{P(k_j^1=l \mid k_j^0=m)})$$

In our project, common drive (visual input from the retina) presents a particular problem. If detection of looming stimuli happens mainly through activation of selected synfire chains, the pattern of this activation would be necessarily synchronized with the causal transfer of excitation from one node to another. Because of that, it cannot be eliminated by methods that rely on the comparison of delays [Wibral 2012, Wollstadt 2014]. Instead, we eliminated the effects of common drive by reshuffling our data, and pairing activation history of each cell with activation history of other cells for reshuffled, unmatching trials recorded in response to same stimulus type. For each experiment, we calculated 1000 randomly reshuffled transfer entropy estimations, and then subtracted the average of these reshuffled TE estimations from our TE estimation, arriving at the value of adjusted TE [Gourevich 2007]:

T’_ij = T_ij - T^shuffled_ij

This approach is similar to the idea of analyzing subtle variations in activation from one response to another, as opposed to the analysis of activation traces themselves. As the stimuli we presented were same in every trial, the progression of the common drive over time was shared across all trials. If a connection between cells i and j was suggested by the analysis of reshuffled data, these cells were clearly sequentially driven by a common input, and not by a true causal connection between them.

For each TE estimation, we also calculated a corresponding p-value, to quantify whether the observed TE was significantly different from the set of TE estimations obtained on surrogate data, corresponding to H0 of fully shared drive, and no causal connections. With the computational power available, we could only afford to generate 1000 surrogate reshuffled networks for every TE calculation, which made it impossible to use the false discovery rate correction on our data, as it is recommended for fMRI-based studies of large-scale brain connectivity [Vicente 2011; Lindner 2011]. With ~10^2 neurons and 10^4 connections the smallest possible non-zero p-value of 0.001, corresponding to finding a more extreme TE value in one out of 1000 surrogate experiments, was already larger than the Benjamini-Hochberg threshold of \frac{k}{m}\alpha=5e-6. With a permissive threshold of \alpha=0.01, each subset of data (recordings of responses to collisions, flashes, and scrambled stimuli), when taken separately, suggested the existence of 2% to 69% of all possible directed edges in the graph, depending on the experiment (median of 8%). The share of edges that were independently discovered in all three types of experiments (median value of 0.1% of all possible edges) was on average 1.8 times larger than one would expect in case of spurious and independent discovery (signrank p=7e-7), suggesting that the three subsets of data, originating from responses to three different stimuli, can be considered replications for the purposes of edge discovery. At the same time, when we tried to restrict our analysis only to edges that were fully replicated in all 3 sets with \alpha < 0.05, we ended up with the median graph size of only 18 edges (6 edges in the largest connected component). The majority (18 out of 19) of datasets with fewer than 10 reconstructed edges were recorded in the early set of experiments, when we used a lower concentration of dye in the solution. As during data acquisition, we alternated between stage 46 and 49 tadpoles, the set of “weak” experiments was not biased, and consisted of 10 younger, and 9 older tadpoles. We decided to exclude these 19 “weak” experiments, and believe that restricting all analysis to remaining 30 experiments did not bias the study.

To make the edge inference more robust, for 30 experiments included in further analysis, we relaxed our criteria on edge discovery, while still giving preference to edges discovered independently in more than one subset of responses. To do so, we included in our reconstruction only edges with geometric mean of p-values below significance threshold: \prod{p_k}<\alpha^3 , where p_k are p-values for each of three subsets of data (responses to flash, crash, and scrambled stimuli). We could not use a fixed significance threshold $$\alpha$$, as dye uptake and focal plane alignment varied from one experiment to another. Instead, we followed a common approach in network analysis and set the average node degree (the ratio of network edges to network nodes, for directed graphs <k>=E/N) to an arbitrary but reasonable value [Stetter 2012]. For this study, we picked a value of 1.0 (number of edges equal to the number of nodes), which lead to 128\pm41 edges in each experiment on average (0.9% of all possible edges); 50\pm21 weakly connected components, and 74\pm30 nodes in the largest connected component. The comparison of network properties (**Fig**) did not change qualitatively in a broad range of assumed average degrees (from ~ 0.5 to 1.5), but observed effects became weaker and regressed to random effects outside of this range.

The TE approach does not distinguish between positive and negative influence of one neuron on another, so our reconstructed edges could include a mix of excitatory and inhibitory connections. To quantify the share of inhibitory connections, we calculated pairwise correlations between activities of individual neurons, compensating for the effect of shared inputs through trial reshuffling (similar to how it was done for TE), and looked at the sign of these correlations for pairs of neurons with TE>0. We found that 3\pm7% of detected connections were inhibitory or inactivating, with no difference between developmental stages (d=0.55, p_t=0.1). According to our current understanding on the tectal architecture, deepest principal tectal neurons are not expected to be inhibitory [REF?], and the share of negative correlations tended to be lower in experiments with better signal-to-noise ratio. We therefore assumed that at least part of inactivating connections may be false discoveries, but as their share did not differ between developmental stages.

For the analysis of **degree distributions**, we TODO

**Network analysis**
We reviewed several lists of statistical and topological metrics of weighted undirected graphs [refs], and selected a sufficiently diverse set of measures that described different aspects of graphs, including average connectivity, unevenness of density, and global structure, and did not change too strongly with the inclusion or exclusion of individual weakly connected nodes. This last point is important, as some topological properties of a graph, such as its cycle order, or the presence of broadly defined “small world” properties, can be notoriously sensitive to the inclusion of only a few weak long-ranged connections [refs]. We wanted to make sure that the measures chosen for network topology description would not change catastrophically from one experiment, or one animal group, to another because of small variations in the level of noise, or a slightly more generous selection of regions of interests during Ca imaging quantification. We ended up with the following list of network metrics:

**Global network efficiency** [refs] was calculated using a function from the Brain Connectivity Toolbox [Rubinov Sporns] on reciprocals for graph weights R_{ij} = 1/w_{ij}, and was defined as:

E = \frac{1}{n} \sum_{i \neq j}^n{\frac{d_{ij}}{n-1}} where d_{ij} is the length of the shortest path P_{ij} connecting nodes i and j: d_{ij} = \sum_{kl \in P_{ij}}{R_{kl}}

**Clustering coefficient** [Fagiolo 2007] was calculated using the Brain Connectivity Toolbox, a function for the weighted directed variety of the clustering coefficient [Rubinov Sporns] according to the formula:

C = \frac{1}{n} \sum_i{\frac {t_i}{(k^o_i+k^i_i)(k^o_i+k^i_i-1)-2\sum_j{w_{ij}w_{ji}}}}

where k^o_i and k^i_i are out- and in-degrees of node i respectively, and t_i is the weighted number of directed triangles that include node i: 

t_i = \sum_{j \neq i}{\sum_{k \neq i,j}{w^{1/3}_{ij}w^{1/3}_{jk}w^{1/3}_{ki}}}

For **spectral modularity**  we used a TODO

Our custom definition of **hierarchical flow** was inspired by [ref], but based on the modified Katz centrality [Katz 1953, Fletcher 2017]. To calculate Katz centrality, we assumed that on average, each node i collected a flow of incoming signals through all edges w_{ij} leading to this node. The activation arriving through edge w_{ij} was proportional to total activation z_j of node j, the edge weight w_{ij}, a normalization coefficient equal to 1/\text{max}*{*kl}(w{kl}), and a damping factor of d=0.9, and also received a small amount of constant activation (1-d)=0.1. The total activation of each node was therefore defined as:

z_i = (1-d) + \fraq{d}{max(w_{kl})} \sum_{j \neq i}{a_j w_ij}

Each node then further redirected this activation to other nodes. This definition is very close to that of pagerank centrality [Page 1999], with a minor difference that the weights are not normalized to the value of total outgoing weights for each node: that is, we work with raw weights of w_{ij} rather than w_{ij}/\sum_k{w_kj}.  It means that a node with many outputs has a strong influence over network activation, nodes with weak outgoing edges act almost as dead-ends. Similar to a standard pagerank algorithm, we found solution the stable solution of this problem iteratively, by initializing the network with equal values of centrality, and then running the equation above 100 times or until convergence. Once the distribution of centralities z_i stabilized, we used the difference between the maximal Katz centrality and mean centrality across all nodes as a measure of hierarchical flow in the network [ref]:

h = max_i(z_i) - mean_i(z_i)

Custom definition of **cyclicity**, or the prevalence of cycles in the graph. TODO

*Graph reshuffling: why and how TODO*
*State our version of Maslov rewiring did not allow self-connections*

We also tested whether the connectivity and position of selective cells within the graph is in any way peculiar, by calculating Pearson correlations between cell selectivity and several different graph centrality measurements. Here we used TODO

To study the co-distribution of node properties (cell selectivity) within the connectivity graph, we used **weighted assortativity** as a measure of non-random association of selective cells into subnetworks. The formula for a mixing coefficient in a weighted directed network is given in [Farine 2014], based on the logic from [Newman 2003] and [Leung 2007]. The original formula from [Newman 2003] for an unweighted undirected graph defines a mixing coefficient as a Pearson correlation coefficient between properties of nodes connected by edges, taken over all edges in the graph:

r=\text{cor}_{ij: a_{ij}=1}(x_i,x_j)

leading to the following expression:

r = \frac{\frac{1}{E} \sum{x_i x_j} - [\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2} {\frac{1}{E} \sum{(x_i^2+x_j^2)}-[\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2}

where sums are taken over all connected edges ij: a_{ij}=1, and E is the total number of edges.

For a weighted graph an equivalent measure can be introduced by replacing summation over edges to summation over all possible pairs of nodes $$ij$$, with weights $$w_{ij}$$ introduced in each sum. The resulting expression can be rewritten in several different ways [Newman 2003; Leung 2007; Farine 2014; Teller 2014], with several alternative bulky expressions ultimately describing a weighted correlation calculated across all connected directed edges ij:

r=\text{cor}(x_i,x_j,w_{ij})

where weighted correlation cor(a,b,w) is introduced through weighted covariances:
cor(a,b,w) = \frac{cov(a,b,w)}{\sqrt{cov(a,a,w) \cdot cov(b,b,w)}}
that in turn are defined as:
cov(a,b,w) = \frac{\sum_i{w_i \cdot (a_i-\bar{a})(b_i-\bar{b})}}{\sum_i{w_i}}

with \bar{a} and \bar{b} representing weighted mean values:
\bar{a}=\sum_i{w_i a_i}/\sum_i{w_i}

Note that this formula differs slightly from the one used in the Brain Connectivity Toolbox [Rubniov].

## Developmental Model

The model consisted of n=81 cells, arranged in a 9x9 grid. The model operated in discrete time, and was run for 500 epochs, 25 time steps each, or for T = 12500 time steps total. Each cell was characterized by three values: its current activity s_i(t) that represented its instantaneous spiking rate; spiking threshold h_i(t) that slowly changed over time, and a constant \hat{s_i} that described the target spiking rate for each cell. The target spiking rates were randomly assigned at the beginning of each simulation, and were distributed normally around 5/n with a standard deviation of 1/n, 

\hat{r_i} = \mathcal{N}(\frac{5}{n},\frac{1}{n})

which means that when these target spiking rates were matched, on average, at any time step, 5 out of 81 cells would be spiking. The spiking thresholds h_i were initialized at the beginning of each simulation with a value

h_i(0) = 1/(n \hat{s_i}) + \mathcal{N}(0,0.1)

where \mathcal{N}(0,0.1) represents a random value, normally distributed around 0 with a standard deviation of 0.1 .

Cells were connected to each other with “synapses” of different strengths, represented by a weight matrix W, with weight 0 \leqslant w_{ji} \leqslant 1 leading from cell i to cell j. At the beginning of each simulation the weights were assigned random values, uniformly distributed between 0 and 1, except for self-connections (loops, w_{ii}) that were set to 0.

At each time step we first calculated the raw activation of all neurons A:

A = WS + B

where W is the connectivity matrix, S is the vector of instantaneous spiking rates s_i , and B is the sensory input (see below). For one cell, we have:

$$a_i(t+1) = \sum_j{w_{ij}s_j(t)} + b_i(t)$$

These raw activation values were then adjusted down, by a formula reminiscent of global feedback inhibition, which helped to avoid run-away activation:

$$a'_i(t+1) = \begin{cases} a_i(t+1), & \text{if } \sum_j{s_j(t)} \leq d \\ a_i(t+1)\Big/ \Big(1 + \big(\sum_j{s_j(t)} - d\big) \cdot \exp(- t/\tau_e)\Big), & \text{otherwise.} \end{cases}$$

Here a’_i(t) is the final, adjusted value of activation for every cell; \sum_j{s_j(t)} is the total sum of all cell activities at the previous time step; d is a constant that sets the level of total activity at which inhibition “turns on”, and that in our case was set to the size of the grid of cells d=9. The exponent \exp(-t/\tau_e) serves as an “easing” function that gradually “eases” the network from inhibition-dominated mode of operation to “free” operation, with a time constant \tau_e=T/7. This “easing” formula was a practical compromise that greatly sped up our computational experiments, as it dampened network activity early on, when network was still close to randomly connected, and so prone to seizure-like activity, but allowed the simulation run on its own later in development. 

The activity of each neuron s_i(t) was then calculated from its total activation a’_i(t):

$$s_i(t) = g_i\big(a'_i(t)\big)$$

using a logistic activation function:
$$g_i(a) = 1/\Big(1+\exp\big(c(h_i(t)-a)\big)\Big)$$

where c is a steepness parameter, set at c=20, and h_i(t) is the current spiking threshold of cell i. At the beginning of each simulation, spiking thresholds h_i(0) were set to random values, uniformly distributed in a narrow band between 1/(n \hat{s_i}) and 1/(n \hat{s_i}_i)+0.1 . During the simulation, the thresholds h_i(t) were updated at each time step, to model the effect of **intrinsic homeostatic plasticity**. For this purpose, for each cell, we kept track of its running average spiking rate \bar{s_i}(t), and updated both average spiking rates and spiking thresholds h_i(t) by the following formulas:

$$\bar{s_i}(t+1) = (1-\kappa)\bar{s_i}(t) + \kappa s_i(t)$$

$$h_i(t+1) = h_i(t) - r_h(\hat{s_i} - \bar{s_i}(t))$$

where \kappa=0.05 is a constant that controls the rate of averaging, and r_h=0.1 is the rate at which spiking thresholds h_i were allowed to adjust, to bring the discrepancy between the target spiking rate \hat{s_i} and running average spiking rate \bar{s_i}(t) to zero.

Once spiking of each neuron at the new time step s_i(t) was calculated, we performed the **spike-time dependent plasticity** (STDP) step, and adjusted synaptic weights w_{ji} linking neurons in the network. The intuition behind STDP in discrete time can be described by the following system, with options 1 and 2 not mutually exclusive:

$$w_{ji}(t+1) = \begin{cases} w_{ji}(t)+\epsilon, & \text{if } s_i(t)\neq 0 \text{ and } s_j(t+1)\neq 0 \\  w_{ji}(t)-\epsilon, & \text{if } s_i(t)\neq 0 \text{ and } s_j(t)\neq 0 \\ w_{ji}(t) & \text{if }s_i(t)=0\end{cases}$$

As in our model neuronal activity s_i(t) was continuous, the non-exclusive system above can be represented by the formula:

$$w_{ji}(t+1) = w_{ji}(t) + r_w \big(s_i(t)w_{ji}(t)s_j(t+1) - s_i(t)w_{ji}(t)s_j(t)\big)$$

or

$$w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w\big(s_j(t+1)-s_j(t)\big)s_i(t)\Big)$$

where r_w is a constant that controls the level of synaptic plasticity r_w = 0.25 .

At the last step, we performed **synaptic competition**, to make sure that TODO

As learning times used in this model were 2 (?) orders of magnitude shorter than those experienced by a tadpole (and ~3 for mammals), we used a special procedure to "ease" the network  into its final state…

WHEN EXPLAINING HOW SELECTIVITY WAS MEASURED The problem of intrinsic plasticity: because neurons in STDP-dominated networks are arranged in ensembles and synfire chains, and so are usually activated together, intrinsic plasticity is no longer just a matter of adjusting individual neuron excitability, but the major process to regulate relative sensitivity to different sensory patterns (assuming that different patterns are detected by different subnetworks). The problem that became so prominent in the model that it may be relevant in the brain as well: to set the synaptic connections right, and to maintain them, each ensemble of synfire chain has to be regularly activated. Yet the more activate it, the less excitable the neurons become, which means that they are less likely to win during competition with either ensemble when actual signals are recognized. Therefore we have a meta-balancing problem (metaplasticity?): if intrinsic plasticity is too flexible, the network that detects unusual, threatening stimuli, will get spontaneously activated in the absence of these stimuli (high false-positive rate), and will quickly habituate to actual stimuli (high false-negative), but will have no trouble maintaining synaptic connections. If intrinsic plasticity is too rigid, the network can maintain “correct” sensitivity (although the question is still open about how exactly to set it), but may have trouble maintaining synaptic connections at rest (no replay). Potential solutions: distinct developmental stages with high and low intrinsic plasticity (sort of like we did in the model, also s47); distinct learning, maintenance, and operation states (sleep? ACh / endocannabinoids? dopamine? serotonin?)

# Acknowledgements

My greatest gratitude is to Carlos Aizenman who encouraged me to publish this work as a single author, even though the calcium imaging experiments were performed in his lab, and the materials were paid for by the money from his grant (NSF IOS-1353044).

Heng Xu (Institute of Natural Sciences, Shanghai Jiao Tong University)
Petko Bogdanov (SUNY Albany)
Joshua Vogelstein (John Hopkins University)
Csilla Szabo (Skidmore College)
Jim Belk (Bard College)
Gerrit Ansmann (Bonn University)
Sven Anderson (Bard College)

# Citations

Fagiolo, G. (2007). Clustering in complex directed networks. *Physical Review E*, *76*(2), 026107.

Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., & Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. *European journal of epidemiology*, *31*(4), 337-350.

Leicht, E. A., & Newman, M. E. (2008). Community structure in directed networks. *Physical review letters*, *100*(11), 118703.

Rubinov, M., & Sporns, O. (2010). Complex network measures of brain connectivity: uses and interpretations. *Neuroimage*, *52*(3), 1059-1069.

Ruthazer, E. S., & Cline, H. T. (2004). Insights into activity‐dependent map formation from the retinotectal system: A middle‐of‐the‐brain perspective. *Developmental Neurobiology*, *59*(1), 134-146.

Tao, H. W., & Poo, M. M. (2005). Activity-dependent matching of excitatory and inhibitory inputs during refinement of visual receptive fields. *Neuron*, *45*(6), 829-836.

Ng, A., Jordan, M., and Weiss, Y. (2002). On spectral clustering: analysis and an algorithm. In T. Dietterich, S. Becker, and Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14 (pp. 849 – 856). MIT Press.

