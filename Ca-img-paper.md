# Ca img paper
Graph analysis of collision detection networks in the tectum, and its replication in a simple computational model

## Abstract

Visual looming is one of the most salient sensory stimuli a seeing animal may encounter, yet mechanisms of looming detection in vertebrates are still purely understood. For many animals, key computations for looming detection and collision avoidance seem to occur in distributed networks in the midbrain, making its hard to tease out the mechanics of these calculations. In this study, we contrast a computational developmental model of the optic tectum with analysis of directed connectivity graphs reconstructed from high-speed Ca imaging recording in Xenopus tadpoles. We report a difference in graph properties of tectal networks reconstructed from younger and older tadpoles, with degree distributions and network modularity changing in development. We also describe that in tadpoles, looming-selective cells tended to receive more inputs, and had higher Katz centrality than non-selective cells. We compare these observations to predictions of a computational model governed by spike-time-dependent plasticity, homeostatic intrinsic plasticity, synaptic competition, and structured visual information. We show that our model network quickly developed robust looming selectivity, and replicated several changes in network properties, including increase in modularity and changes in degree distributions. Some other changes predicted by the model (hierarchy, efficiency, spatial distribution of selective cells) were however not observed in biological experiments. Finally, by comparing several alternative models, we show which developmental rules were most critical for the development of looming selectivity.

# Introduction

Few sensory stimuli are as conspicuous and ill boding as visual looming. A retinal projection that is small, but quickly grows in size, may promise either a painful collision, or a meeting with a hungry predator, and so it inherently calls for an action, such as an avoidance maneuver, freezing, defensive posturing, or a blink. Moreover, looming detection has to be fast to be meaningful. Not surprisingly, it is described in virtually every type of animals that uses vision, from insects to primates [Pereira 2016].

In different animals, looming detection seems to rely on a variety of networks and solutions [Frost Sun 2004], including dimming detectors in the retina [Ishikane 2005; Munch 2009], opponent motion detection [Klapoetke 2017], and competitive spike-frequency adaptation [Perron Gabbiani 2009; Fotowat Gabbiani 2011]. Even within an single clade of anuran amphibians (frogs), animals seem to employ several competing looming detection mechanisms, such as non-linear detection of retinal oscillations [Baranauskas 2012], and rebound of recurrent activity [Jang]. Moreover, it seems that at least some of these competing mechanisms may lead to different subtypes of avoidance responses, as it was described in insects [Card Dickinson 2008; Chan Gabbiani 2013], fish [Burgess Granato 2007; Portugues 2009; Budick 2010; Temizer 2015; Bhattacharyya 2017], and tadpoles [Khakhalin 2014].

While this co-existence of multiple alternative solutions for looming detection may seem overwhelming, it actually matches the recent insight from the field of machine learning. Simple, crude ways of detecting important features of sensory stimuli are critical for training more sophisticated and efficient networks, that are then used for more nuanced analysis of the sensory world at later stages of development [Marblestone 2016]. In case of looming stimuli in tadpoles, early in development, when collision detection is weak [Dong], animals may use “hardwired” dimming receptors in the retina to detect collisions, but also to “bootstrap” more sophisticated motion-dependent networks in the tectum. Later, these motion-dependent networks can serve as a first line of defense, identifying early phases of looming, and providing information to motor neurons in the hindbrain to perform course correction [Khakhalin 2014; Bhattacharyya 2017], yet dimming detectors could remain as a backup, and mediate a more urgent and less coordinated response. Moreover, every time collision avoidance is not performed perfectly, these sensorimotor networks can be further refined, relying on inputs from the lateral line and mechanosensory detectors in the skin [Felch].

Traditionally, the golden standard for a mechanistic explanation was to demonstrate that activation of a certain structure is sufficient and required for a behavior [Krakauer 2017]. This reductionist approach works well in some systems, such acoustic startle detectors [Korn Faber 2005], or central pattern generators in the spinal cord [Roberts Soffe 2010], that are compact and isolated enough. Yet most systems in the brain act as complex, interconnected, distributed dynamical systems, which makes it hard to represent them as a sum of “parts”, with distinct functions ascribed to each of these parts [Gao 2015]. It is possible to study these complex systems statistically, identifying properties that hold “on average”, and differ in functional and dysfunctional networks, but a statistical approach typically does not grant true insight into the “meaning” of these properties [Bassett 2017]. For example, knowing that a disordered brain does not adhere to the same statistics as a normal brain would not necessarily tell us how to “fix” this brain, even if we could rewire individual neurons in a targeted fashion.

A more promising approach is to study the origin of functional structures in the brain. Most neural networks are not truly hardwired, but dynamically “evolve” from a set of developmental rules [Pietri 2017], similar to how it happens for Turing patterns or fractals [Lefevre 2010; Bullmore Sporns 2012], and may also rely on streams of structured sensory information coming from the external world [Gao 2015]. This insight is encouraging, as the agents that follow these developmental rules are individual cells and their compartments, such as active dendrites and synapses, which implies that while the ultimate product of these rules may be exceedingly complex, the rules themselves have to be relatively simple [Bassett 2018]. It may be therefore, that our best chance to truly “understand” the brain lies in studying the sets of rules that lead to the development of functional networks [Linderman 2017]. This approach proved to be extremely fruitful recently, as several complex network phenomena, including visual receptive fields [ref], grid cells [ref], and decision circtuis [refs; Haesemeyer .. Engert 2018], were shown to evolve spontaneously in systems governed by intuitive developmental rules and clear behavioral goals. 

In this paper, we look at the structural correlates of looming selectivity in the developing optic tectum of Xenopus tadpoles. Tadpole tectum offers a unique opportunity to study the dynamics of sensory integration, as its neurons are excessively plastic [Pratt; Busch], are strongly connected to each other [James, refs], and develop highly reliable looming selectivity within about a week, as tadpoles mature from developmental stage 46 to stage 49 [Dong, Khakhalin]. The refinement of tectal connectivity is dominated by spike-time-dependent plasticity [Poo, more], which is known to favor the development of synfire chains [refs]: ensembles of neurons that are activated in a sequence [ref], and selectively respond to certain patterns of temporal activation [Clopath 2010]. Because tadpole tectal neurons spike relatively slowly, with broad spikes and long refractory periods [Ciarleglio; Jang], these synfire chains are expected have delays of about 10 ms from one neuron to another [Ciarleglio], compared to 2 ms in the mammalian cortex [ref]. This slower signal propagation brings intratectal connections to the threshold of direct detectability by fast Ca imaging techniques that operate at rates of ~100 frames/s, allowing us to observe not just co-firing of neurons within an ensemble or clique [refs including correlation papers and recent cliques [Avitan 2017], but a propagation of signal through these ensembles.

We used high-speed Ca imaging data to infer connectivity patterns in small sub-networks within the tectum, described their in younger and older tadpoles, and compared them to connectivity patterns predicted by a simulation model. We show that our model successfully predicts several aspects of topology and functionality in looming-sensitive networks. From this, we conclude that the architecture of tectal networks may be explained by an interplay of simple developmental rules with structured visual information coming from the eye. Finally, we gradually deconstruct our model, and show which developmental rules are most critical for tectal network development.

# Results

For all mathematical methods used in this paper, in the main text we provide their names, and the interpretation of the results, while definitions and details are given in the “Methods” section. In all statistical analyses, we report p-values without correction, and interpret them according to Fisher, and not Neyman-Pearson philosophy [Greenland 2016]. This approach is preferable for this study, as many of our analyses are not independent, but are also not redundant, and rely on different metrics and H0 hypotheses. At the interpretation step, we pay more attention to hypotheses supported by several alternative analyses, pointing in the same general direction.

We performed Ca imaging recordings from 14 stage 45-46 tadpoles, and 16 stage 48-49 tadpoles, recording responses from 128±40 tectal cells (between 84 and 229). Here and below, “±” after the mean denotes standard deviation, and unless stated otherwise, these sample sizes (n=14 and 16 animals for stage 46 and 49 tadpoles respectively) apply to all analyses between younger and older animals. To each tadpole, we presented a sequence of three different stimuli, always in the same order: a looming stimulus, followed by a full-field flash, followed by a spatially “scrambled” looming stimulus. Scrambled stimuli were identical to looming, except that the visual field was split into a 7x7 grid of square tiles, and these tiles were randomly rearranged in space. In total we presented 60±11 stimuli to every animal (60 or more stimuli in 28/30 cases), which means that a stimulus of every type was presented at least 20 times. High speed calcium imaging recordings were performed from one layer of “deep” principal tectal neurons in the tectum [Xu; Truszkowski]; from these videos we then extracted fluorescence traces, and inferred average spiking of each neuron within every frame.


## Responses and stimulus selectivity

TODO *Typical shape of responses across all brains.*

The **total spiking** **output** of observed tectal networks tended to be higher in response to looming stimuli than to flashes (39±29% higher for younger, and 25±25% higher for older tadpoles). There was no change in overall preference for looming stimuli in development (pt=0.15). The total tectum output was variable from one experiment to another, and while the majority of preparations showed a significant selectivity for looming stimuli (p_t<0.05 in 11/14 and 9/16 experiments for stage 46 and 49 tadpoles respectively), the remaining 10 experiments showed no significant preference for looming stimuli. Note that the majority of experiments with no full-brain preference for looming stimuli were from older, and so presumably more developed animals.

The difference between responses to looming and scrambled stimuli was even more diverse: some preparations did not have a preference between the stimuli (9/14 and 7/16 for younger and older tadpoles respectively), some preferred looming stimuli to scrambled (1/14 and 4/16), while some significantly preferred scrambled stimuli to looming (4/14 and 5/16). This individualized preference for either looming or scrambled stimuli was most likely an artifact of our experimental protocol, as “scrambled” stimuli were uniquely rearranged for each experiment, but were kept constant within each experimental session. It seems that this design produced less salient stimuli in some experiments, but more salient stimuli in others. These results also support our earlier reports that in tadpoles, the total tectal respond more strongly depends on the dynamics of visual stimuli, rather than on its geometry [Khakhalin 2014, Jang 2015].

To quantify **selectivity of individual tectal cells**, we used Cohen’s effect sizes of their responses to stimuli (the difference in mean responses, divided by the pooled standard deviation of response amplitudes). We calculated two different measures of selectivity: that for looming stimuli over full-field flashes (a type of selectivity that can rely on both stimulus dynamics and its spatial organization), and looming over scrambled stimuli (that can only rely on spatial organization, as both stimuli shared the dynamics). On average, tectal cells showed selectivity for looming stimuli, with no change from younger to older tadpoles (0.67±0.50 and 0.46±0.47 respectively, pt=0.3). The share of cells that responded to looming stimuli stronger than to flashes also did not change in development (84±23%, 77±21%; pt=0.4). There was however a change in the distribution of cell selectivities within individual networks: while the variance of selectivity values within each brain did not change (pt=0.3), there was a change in skew (pt=0.02), which was positive in younger animals, but negative in older ones. The skew decreased because the difference between the top-selective (90th percentile) and median selective cells within each brain became lower in older animals: 0.75±0.26 in younger, but 0.53±0.27 in older animals (pt=0.03). Both of these results are unexpected: stage 49 tadpoles perform better than stage 46 tadpoles in collision avoidance tests [Dong], and so we expected them to have a higher selectivity to looming stimuli, and also to have a more well-defined subset of looming-selective cells, as described in adult frogs [Nakagawa 2010; Baranauskas 2012], and other vertebrates [Wang Frost 1992; Wu 2005; Liu 2011]. Yet in our experiments overall selectivity for looming stimuli did not change, and a subpopulation of strongly selective cells became less prominent in older animals.

We then considered the second, more computationally demanding definition of selectivity: a preference for spatially organized looming stimuli over scrambled stimuli. On average, tectal cells did not have a preference between these two stimulus types (average selectivity of -0.07±0.33 in younger tadpoles, -0.04±0.49 in older ones), and the value of average selectivity also did not change in development (pt=0.9). The share of cells that responded to looming stimuli stronger than to scrambled stimuli was at a chance level for both developmental stages (46±31%, 48±37%, pt=0.9). Similarly, there was no change in either within-brain variance of this selectivity (pt=0.9), or the 90-50 percentile asymmetry of values (pt=0.8). 

The selectivity for scrambled stimuli over flashes correlated with selectivity for looming stimuli over flashes in both developmental groups: within-brain r=0.82±0.13, p1t=3e-12 for younger animals, and 0.75±0.18, pt1=3e-11 older ones; no change in development (pt=0.3). On the contrary, the preference for looming over flashes did not correlate with preference for looming over scrambled stimuli (r=0.03±0.29, p1t=0.7 for stage 46; 0.13±0.30, p1t=0.1 for stage 49). This further demonstrates that during collisions, the majority of cells in the tectum responded to stimulus dynamics, rather than to its geometry.

To assess the cell-to-cell variability of responses, we performed **exploratory factor analysis** (principal component analysis, followed by promax rotation) of cell responses within each brain. We restricted this analysis to responses to one stimulus type at a time, as otherwise factor analysis detected cell-to-cell differences in stimulus selectivity, while we wanted to look at the differences in dynamics within a family of responses. For looming stimuli, the 1st and 2nd principal components explained on average 19%±7% and 4%±1% of variance in younger tadpoles, and 24%±14% and 3%±1% in older tadpoles, and the biggest variance in cell response profiles, according to factor analysis, was in response latency (**Fig**). Moreover, in all experiment early responding cells tended to group together in one part of the visualized field (**Fig**). In each experiment, we found the “center” of this group of early-responding cells, by optimizing the correlation between the distance of each cell to this center, and the relative score of the early component for this cell (**Fig**). The fit was robust and unambiguous, with linear fit significant (p<0.05) in every experiment (30/30), and average correlation of r=0.59±0.23. As we were looking for the “best possible fit” for each cell configuration, if there were no effect (H0), we would have still observed significant correlations in more than 5% cases, so we reshuffled cell identities for each experiment 5 times, and observed average r of 0.13±0.06 and p_r<0.05 in 21% of experiments. From that we concluded that observed spatial organization of early responses to looming stimuli was real, and not just an artifact of flexible analysis, and interpreted it as a direct observation of a retinotopic map in the tectum [Ruthazer 2004]. For each cell, we estimated the latency of its average response, and this latency positively correlated with the distance from the “retinotopy center” (average r=0.35±0.24, individually significant in 25/30 experiments). Curiously, while visual projections to the tectum are known to be actively remodeled in development [XXXXXX], there was no difference in the precision of the functional retinotopic map between younger and older tadpoles, similar to how it was described for zebrafish [Avitan 2016], as best fit correlation coefficients did not increase in development (r=0.63±0.21 and 0.57±0.25 respectively, p_t=0.5).

Knowing where our looming stimulus is projected, we could then check whether looming-selective cells were more likely to be found in the center of the expanding activation area (as it would be expected if collision detection was local and retinotopic [Sun Frost?], or if it was primarily based on feedback excitation [Jang Khakhalin]), or at the periphery (as predicted by the synfire model [refs]). We found that selectivity for looming stimuli over flash tended to decrease with distance from the estimated projection center (**Fig**) for both stage 45 (average r=−0.37±0.27; individual correlations significant p_r<0.05 in 12/14 individual experiments), and stage 49 tadpoles (average r=-0.09±0.35; individual p_r<0.05 in 12/16 experiments), indicating that looming-selective cells tended to be located in the center of the emerging spatial response. Similarly, in both younger and older tadpoles, selectivity decreased with response latency (stage 46: p_r<0.05 in 11/14 animals, average r=-0.29±0.11; stage 49: p_r<0.05 in 10/16 animals, average r=-0.16±0.21). Both correlations were weaker in older tadpoles (p_t=0.02 for distance-selectivity, p_t=0.03 for latency-selectivity), indicating a more uniform distribution of looming-selective cells within the network.

Finally, as a most holistic way to quantify overall tectal network selectivity, we looked at the **total predictive power** of tectal responses [Avitan 2016]. To do so, we ran a logistic regression on one half of data, linking the total response of each recorded tectal cell in each trial to the visual stimulus type used in this trial. Then we measured the quality of this linkage on the second half of recorded data. The quality of prediction was rather low: 59%±12% for younger, and 62%±13% for older tadpoles, with no change in development (p_t=0.6).

## Variability and ensembles

We then assessed the trial-to trial variability of tectal responses, to see whether it changed in development, as it was previously reported for variability of spontaneous activity [Xu 2011; Avitan 2017]. We used the principal component analysis of response waveforms, and looked at the total number of components that was needed to describe 80% of variability in the data [Avitan 2017]. We found that this number was similar in young and old tadpoles, but showed a minor increase in response richness in older animals (insignificant for each stimulus alone, but consistent across stimuli): 51±14 and 65±28, pt=0.1 for responses to looming stimuli; 49±12 and 62±32, pt=0.2 for flashes; 51±14 and 64±26, pt=0.1 for scrambled stimuli. 

To better describe the structure of network activation during sensory responses, we used spectral clustering technique to identify ensembles of tectal cells that tended to be either active or not active together on a trial-by-trial basis. Unlike for recordings of spontaneous activity, we could not easily aggregate activity states into clusters [Avitan 2017], as the states of our networks were driven by shared and repeated sensory inputs. Instead, we subtracted normalized average responses of each cell from its responses in individual trials, and calculated pairwise correlations of the remaining “anomalies” of trial-by-trial activation for every cell (**Fig**, see “Methods”). We turned these pairwise correlations into pairwise distances in a multidimensional space, and ran a series of spectral clustering partitions [Ng Jordan Weiss 2002], assigning cells to different clusters. Finally, of all possible partitions, we picked a partition that maximized spectral modularity on a weighted graph of pairwise cell-by-cell correlations [Gomez 2009; Newman 2006]. We found that the number of ensembles was not significantly different between younger and older tadpoles (10±5 in stage 45, 11±11 in stage 49; pt=0.9; **Fig**), but in older tadpoles ensembles were slightly less isolated from each other, producing lower values of network modularity (0.14±0.05 to 0.09±0.06, pt=0.03; **Fig**). This implies stronger coordination between the activity of ensembles in older tadpoles.

The tectal ensembles revealed by this method tended to be spatially localized within the brain, rather than distributed across the network (**Fig**): and cells that shared an ensemble were on average closer to each other than to cells from different ensembles, for both younger (29±9% closer) and older tadpoles (25±10% closer; no difference in development pt=0.3).

## Network reconstruction

The high speed of video acquisition used in this study (83 frames/s) allowed us to look not just at synchronous correlations between the activity of individual neurons, but at the propagation of spiking signal through the network. In Xenopus preparations, it takes about 5 ms for a presynaptic action potential to elicit release in a typical tectal synapse [Khakhalin 2012; Ciar 2015], and about 10-20 ms for a model synaptic current to depolarize postsynaptic neuron above the spiking threshold [Ciar 2015; Busch 2017], which makes the frame-to-frame delay in our video recordings (12 ms) ideally posed to infer neuronal connections from activity.

To reconstruct network connectivity, we used the transfer entropy approach [Gourevitch 2007; Stetter 2012, more refs]. Intuitively, for each pair of neurons i and j, we quantified the amount of additional information that the past activity of neuron i can provide to predict the current activity of neuron j. This is somewhat similar to calculating a cross-correlation between the activity of neuron i and the activity of neuron j in every next frame, but unlike for a correlation, transfer entropy approach does not make assumptions about the nature of the influence neuron i has over neuron j. It means that in a general case, TE has a higher power to detect causation from activity [Stetter 2012, more refs].

In our experiments, all tectal neurons received shared inputs from the eye, which recruited them in a similar sequence in each trial. This complicated connectivity inference, as neurons could spike in a sequence both because they were connected, and because they received innervation from sequentially activated areas of the retina. To compensate for this, we randomly reshuffled trials for every neuron, calculated average transfer entropy on reshuffled data, and subtracted this value from the transfer entropy on trial-matched data [Gourevitch 2007, some Wollstadt] (see Methods for details). In essence, instead of analyzing raw responses, we analyzed small deviations from the average response, and quantified whether these deviations tended to propagate through the network, from one neuron to another. The reshuffling step also allowed us to calculate a p-value for each pair of neurons, which quantified how extreme the observed value of transfer entropy for this pair of neurons was, compared to a value that would arise from shared inputs, but no causal connections. 

We interpreted the table of transfer entropy values as an approximation of a weighted adjacency matrix W that described connections between tectal neurons, with element $$w_{ji}$$ describing the strength of connection from neuron i to neuron j. We calculated W and corresponding p-values independently on looming, flash, and scrambled stimuli, and then used these independent estimations to ensure some level of internal replication within each experiment (see Methods). 1.6±1.4% of all edges were replicated between stimuli, which was significantly higher than 0.6±0.09% as expected if edge discoveries were random (paired t-test p_t=9e-9). Finally, in each experiment, we introduced cut-offs on $$w_{ji}$$ and p-values, to eliminate weak and noisy edges from the connectivity graph. We automatically adjusted these cut-offs in every experiment, ensuring that the average node degree for each network (the average number of connections per node) is close to 1.0. It means that in each network, the total number of edges in the graph was equal to the number of nodes (recorded cells); an approach frequently used in noisy network analysis [Stetter 2012, more refs]. With the average total degree set at 1.0, the effective significance thresholds on p-values were between 0.001 and 0.007 (median 0.004). 

The simplest statistical property of a probabilistic networks is its **degree distribution**: the share of nodes with different number if incoming (k_in) and outgoing (k_out) connections. We compared rounded degree distributions (see Methods) between networks detected in younger and older tadpoles (**Fig. X**), and found that more mature networks contained fewer unconnected cells (k_in=0, p_t<0.03) and fewer cells with high number of connections (k_in=5, k_out=6, p_t=0.01 in both cases), but more cells with intermediate number of connections (k_in=2, p_t=0.001, and k_out=2, p_t=0.04). If we approximated degree distributions (excluding k=0) with a power law, as would be true for a scale free network (**Fig. X**, see Methods), the distribution constant \minus \gamma was smaller in younger (1.48±0.19) than in older tadpoles (1.82±0.25, p_t=2e-4). The distribution of degrees became sharper, with a steeper drop between the occurrence rate of weakly connected and highly connected cells. **Fig. X** shows an exaggerated illustration of what this difference in degree distributions mean for the network topology: developed tectal networks are closer to **Fig. Xb**, with “egalitarian”, linear chains of connected neurons (degree k=1) and forks (k=2), while younger neurons have more hyperconnected hubs (k>5) and unconnected nodes (k_in=0), similar to **Fig. Xa**. 

An unusual feature of our calcium imaging protocol, compared to most common calcium imaging techiques, is that the signal-to-noise ratio varied greatly from one cell to another, depending on how far its cell body was from the focal plane, and how much dye it absorbed during staining, depending on the share of surface exposed to the chamber solution. As a result, the number of cells with weak signals, that had low values of pairwise transfer entropy, and so appeared unconnected, was affected by the physical curvature of the tectum. While we aimed to minimize this curvature, and maximize the intersection between stained cells and the focal plane, we wanted to minimize the effect of poorly resolved cells on our analysis, and so restricted it to the largest weakly connected component of the network. There was no difference in the number of weakly connected components detected in younger and older tadpoles (50±14 and 50±26 for stages 45 and 49 respectively, N=14 and 16; p_t=0.9), but in older animals the largest weakly connected component included a higher share of observed cells (50±6% and 64±12% respectively; p_t=4e-4), which matches the change in degree distribution, as described above.

Synaptic connections in the tectum exhibit robust spike-time dependent plasticity (STDP) [Poo papers], and a known effect of maturation in networks dominated by STDP is that with time neuronal connections become highly asymmetric. Indeed, if cells i and j are reciprocally connected, every time j spikes after i, STDP would increase weight $$w_{ij}$$ , but diminish weight $$w_{ji}$$ [Abbott and other refs]. We found that in our data, the share of bidirectional edges (with both w_{ij} and w_{ji}>0) among all detected edges was smaller (0.3$$\pm$$0.3%) than expected for random edge rearrangement in graphs of our size (0.4$$\pm$$0.1%, paired p_t=0.02), indicating asymmetric information flow in the tectum. Moreover, the share of bidirectional edges decreased in development, from 0.4$$\pm$$0.3% in younger animals to 0.2$$\pm$$0.2% in older animals (p_t=0.03), suggesting that STDP is shaping emerging network topology at these developmental stages.

We then looked at whether connected cells were more likely to be located closer to each other in the tectum. We found that the average distance between connected cells was indeed shorter than the average distance we would get on a randomized graph: 18$$\pm$$10% shorter for stage 45, and 17$$\pm$$8% shorter for stage 49 tadpoles (individually significant with p_t<0.05 for 13/14 and 16/16 experiments respectively). Contrary to our expectations, and in contrast to what is known about visual inputs to the tectum [Tao Poo 2005; refs], the intra-tectal connectivity did not become more compact in development (pt=0.7). This may suggest that tectal networks rely on far-reaching recurrent connections to integrate visual information across the visual field [Baginskas 2009; Liu Pratt; Jang]. 

## Network properties

We then measured several standard network properties for every connectivity graph reconstructed from tadpole tecta. For each age group, we used two different approaches to check whether the values of these network properties were statistically unusual. First, we compared values actually observed in the network to values obtained on a set of randomized graphs, in which the distribution of edge weights w_ij, and the number of edges adjacent to each node (unweighted in- and out- degrees) were kept fixed (a procedure known as degree-preserving rewiring [Maslov 2002]). Second, we repeated this calculation, but this time also randomized node degrees, by reassigning existing edges among all possible edges in a graph (simple rewiring). Finally, we compared network statistics for younger and older tadpoles to each other, to see whether any network properties changed in development. In theory, the use of two different comparisons with randomized graphs would allow us to explain any change in network properties by a combination of two different effects: the change in node degree distribution, and a change in the large-scale structure of the graph [Angsman]. The summary of all comparisons is given in **Table 1**.

The measure of large-scale connectivity in the graph, named weighted network **efficiency**, is defined as the average inverse shortest path, for any two nodes in the network [Rubinov 2010, Latora Marchiori 2001]. This value is high when, on average, there is a short path between any two randomly chosen nodes, and so signals can be easily propagate within the graph; the value is low when some nodes are located far from each other on a graph (**Fig. example)**. Traditionally, this measure interprets graph edges as pairwise distances between the nodes, with high value edges representing weak connections; for a graph of weights however, high value edges represent strong connections, so network efficiency is calculated on inverse weights R_ij = 1/w_ij [Rubinov 2010]. In our experiments, network efficiency (0.004±0.002 for stage 46, 0.002±0.002 for stage 49 tadpoles) was slightly lower than expected for a random network with matching degree distribution (d=-0.3, paired p_t=0.04 and d=-0.3, paired p_t=0.06 for younger and older tadpoles respectively), or a fully randomized network with the same distribution of edge weights (d=-0.5 and -0.05; paired p_t=0.02 and 0.01 for younger and older tadpoles respectively). Efficiency did not seem to change in development (d=-0.8, p_t=0.06).

Network **clustering** coefficient describes the small-scale heterogeneity in the network [Fagiolo 2007], and is defined as the relative frequency of two neighboring nodes forming a triangle via a third node that is connected to both of them (**Fig. example**). The value of clustering coefficient in our networks was small (2.4±2.5 e-3 for stage 46, 1.5±1.6 e-3 for stage 49 animals), but was still slightly larger than expected in a rewired network with same degree distribution (d=0.5 and 0.6, paired p_t=0.01 and 0.02 for younger and older animals), and similar to what would be expected for a random graph (d=0.1 and 0.2; paired p_t=0.4 and 0.3). This suggests that the observed distribution of degrees made the network less clustered than a random network, but the assortative arrangement of nodes countered this effect. There was no change in clustering in development (d=-0.4, p_t=0.3, **Fig.**).
 
The network **modularity** is the most commonly used measure of large-scale network heterogeneity [Leicht 2008, Newman]. A network with high modularity can be split into a set of subnetworks that are connected within themselves stronger than expected on average, while the connections between these subnetworks are weaker than average for the graph (**Fig. example**). In our experiments, networks in both younger and older animals were not significantly less ore more modular than expected for a randomized network with the same degree distribution (d=0.2 and 0.3, paired p_t=0.2 and 0.06 respectively). Randomized networks with matching degrees, however, were significantly less modular than a fully randomized network (d=-0.6 and -0.4, paired p_t = 3e-7 and 5e-5 respectively), suggesting that observed functional connectivity networks were less modular than a random network, but this difference in modularity was fully explained by the distributions of node degrees. The network modularity also increased in development (d=1.0, p_t = 0.01), and this change was mediated by a difference in edge weights distribution, as the effect did not disappear if networks were randomly rewired before comparison (for permutatoin rewiring, d=1.2, p_t=0.003).

The property of **hierarchical flow** is a rather complicated measure [XXX] based on the distribution of Katz centrality values within the network (see Methods for the definition) [XXX]. Intuitively, the flow hierarchy is high when connections between different subsets of nodes largely point in the same direction, as it happens in layers networks, networks with chains of directed edges, or activity sinks [Mones Vicsek Vicsek 2012]. We hypothesized that a network with dedicated looming detectors may exhibit non-trivial flow hierarchy, but the data we observed was hard to interpret, as structural non-randomness and the non-randomness of degree distribution affected hierarchical flow coefficients in opposite ways, with these two effects cancelling each other. Indeed, observed networks were more hierarchical than randomized networks with matching degrees distribution (d=1.5 and 1.1, paired p_t=1e-04 and 1e-03 for younger and older tadpoles respectively), but randomized networks with matching degrees distribution were substantially less hierarchical than fully randomized networks with matching distribution of w_ji (d=-4.5 and -2.6, paired p_t = 4e-8 and 2e-6 respectively). There was no difference in development (d=-0.3, p_t=0.4).

We also introduced a measure of **cyclicity**, which quantified the presence and average strength of various directed cycles starting from a node and returning back to the same node, relative to a similar value on a full graph without self-loops, with the same number of nodes (see Methods). The cyclicity of observed graphs (1.4±1.4 e-4 for younger, 3.8±4.3 e-5 for older tadpoles) was higher than expected for this degree distribution (d=0.8 and 0.8, paired p_t=0.002 and 0.01 for younger and older animals respectively), but the value for a random graph with a matching degree distribution was lower than for a fully randomized graph (d=-2.2 and -2.6, paired p_t=3e-5 and 2e-6 respectively). This non-trivial pattern means that compared to a fully random graph, the distribution of degrees did not favor short cycles (as it had fewer nodes with d>2), yet actually observed graphs were structurally enriched with short cycles. There was no change in cyclicity in development (d=-0.1, p_t=0.9).

## Selectivity Mechanisms

Regardless of the exact architecture of the collision-detecting networks in the tectum, it is safe to assume that looming selective neurons integrate multiple streams of information coming from different parts of the visual field. We therefore hypothesized that looming-selective neurons may be non-randomly located within the detected connectivity graph. For example, they could have received more incoming connections, on average, compared to non-selective cells. To get a glimpse of possible looming selectivity mechanisms, for each reconstructed network we calculated correlation coefficients between looming selectivity of each neuron and several values that quantified its location within the graph; then looked whether these correlation coefficients were significantly different from zero across experiments. 

For the number of incoming connections, we found that in stage 46 tadpoles, selectivity did not correlate with weighted in-degree (r=0.08±0.24, p_t1=0.2; individual r>0 in 9/14 experiments), but in stage 49 tadpoles the correlation was significant, even if small (r=0.07±0.13, p_t1=0.03; individual r>0 in 12/16 experiments), indicating that in older tadpoles selective cells received slightly more incoming connections, compared to non-selective cells.

As a slightly more advanced way to quantify information sinks, we calculated the **Katz centrality** measure for each of the neurons [refs]. Katz centrality is similar to pagerank centrality, but better represents signal propagation in a neural network, and also better generalizes to weighted graphs [refs]. Nodes with high Katz centrality have many paths leading to them, so a spike originating at random within a graph is more likely to eventually excite a cell with a high value of Katz centrality, compared to a cell with a low value. Similar to the result for in-degree, we found that in younger tadpoles, there was no correlation, on average, between Katz centrality and cell selectivity (r=0.08±0.25, p_t1=0.25, individual r>0 in 8/14 experiments), while in older tadpoles Katz centrality correlated with selectivity (r=0.08±0.13, p_tq=0.03; individual r>0 in 12/16 experiments).

As cells with higher Katz centrality tend to be activated more often, we checked whether cell selectivity for looming stimuli correlated with its average activity during the recording. We found that for both younger and older animals, actively spiking cells tended to have stronger looming selectivity (r=0.20±0.24 and 0.14±0.19 respectively, in both cases r is significantly >0, p_t1=0.01 and 0.01). There was no change in this correlation over development (p_t=0.4).

If looming-selective cells are gathering information from the network, it was also plausible to hypothesize that they would be more likely to be connected to each other, rather than to non-selective cells. To test whether it is true, we used the weighted **assortativity of selectivity** values: essentially, a correlation between the selectivity scores of every pair of cells connected by an edge, with a strength of this edge weighted into the calculation. Similar to the tests described above, the selectivity values of connected cells did not correlate in graphs reconstructed from younger tadpoles (r=0.05±0.12, p_t1=0.14, individual r>0 in 9/14 experiments), but they did correlate in graphs recorded in older tadpoles (r=0.05±0.07, p_tq=0.01; individual r>0 in 11/16 experiments). Together, these results suggest that the distribution of selective cells in developed network becomes less random, which may mean that distributed network calculations start to play a bigger role in looming stimulus detection in older animals.

We also hypothesized that on average selectivity for looming stimuli may gradually “accumulate” as the signal propagates through the network, and so “downstream” cells on the receiving end of a strong edge within the graph would, on average, be more selective than “upstream” cells. This hypothesis proved to be wrong in both younger and older tadpoles: across all experiments, 52±7% of strong edges (top quartile of w_ji values) led from cells with less to more selective cells, which is not different from chance rate (p_t1=0.1 for analysis across experiments, n=30). A weighted average increase in selectivity between two cells connected by an edge was 0.02±0.07 (p_t1=0.2, n=30), and there was no change in this value in development (p_t=0.2, n=14, 16).

For the sake of transparency, here’s a list of measures that we calculated, but could not interpret, and so did not include in the final manuscript: four measures of weighted directed degree assortativity (in-in, in-out, out-in, and out-out); pagerank centralities; Katz centralities and hierarchical flows on reversed graph W’.

# Model

As tectal networks we studied are complex, distributed, and noisy, our experimental results defied easy mechanistic interpretation, especially as in some cases we used several alternative ways to quantify an intuition, and got conflicted results. For example, for statistical network properties (**Fig**), the effects of non-random degree distribution and large-scale graph structure often affected network properties in opposite directions, leading to small total effects. Similarly, when we tried to quantify the “integrative” position of each cell within the connectivity graph, only some of these measurements correlated with cell selectivity for looming stimuli, and it was not clear which of these measurements are objectively “better”, in terms of encapsulating the true properties of the network. Moreover, the sample sizes we have to work with are relatively small (14 and 16 observed networks for two developmental stages, respectively), and and only allowed us to detect large changes in network properties (d \approx 1.0, assuming p_t<0.05 and 80% power), which further complicated the interpretation of results.

To compensate for these limitations, and to provide a theoretical counterpoint to our observations, we created a mathematical model of the developing tectum, and ran this model through the same set of measurements that we applied to our experiments. The model consisted of 81 artificial neurons, arranged in a 9 by 9 grid, that were originally all connected to each other (every neuron to every neuron) with random positive (excitatory) synaptic weights (**Fig**). The model operated in 10 ms increments, and we interpreted the output of each neuron as its expected spiking frequency. At each time frame we looked at the activity of the network in the previous frame, calculated the synaptic inputs each neuron would send to each other neuron, and summed them up to calculate the total activation of each neuron. We then used a sliding logistic function to estimate the probability of spiking in postsynaptic neurons, in response to this total activation. See “Methods” for the formulas, and the detailed description of the model.

To allow the network develop in time, we introduced three simple developmental rules: spike-time-dependent plasticity (STDP), homeostatic plasticity, and synaptic competition. Our implementation of STDP approximated biological STDP, as observed in the tadpole tectum [Poo]: if two cells were active in two consecutive 10 ms time frames, and they were connected with a synapse, the weight of this synapse was increased. Conversely, if two cells were active within the same time frame, the weight of a synapse connecting them was decreased, as it means that one cell would try to activate another cell after it already spiked. The homeostatic plasticity adjusted the excitability threshold of each neuron, trying to keep its spiking output constant on average [Turrigiano; Pratt 2008]. The synaptic competition attempted to keep the total strength of synaptic inputs to each neuron, as well as the total strength of synaptic outputs from each neuron, close to constant, by scaling all other synapses of both input and output neurons down every time one synapse increased in efficiency [Cohen-Cory 2002; Munz 2014; Hamodi .. Pratt 2016].

With these three rules at play, we exposed the model to patterned sensory stimulation, modeling retinotopic inputs from the eye. For the main series of computational experiments, we hypothesized that in biological networks, STDP-driven changes may be amplified by a learning signal [refs on latent learning?], coming either from dimming receptors in the retina [refs], or mechanosensory systems in the hindbrain [refs]. Therefore, in this set of experiments we only exposed our model to looming stimuli (but see below for sensitivity analysis). The network was allowed to develop for 12500 time steps (500 slightly variable looming stimuli), and we saved its topology at five equally spaced time points during this process, from a naive network, to its final state. We ran independent simulations 50 times, and for each network snapshot we analyzed the properties of the connectivity graph, as well as the responses of each network to “model visual stimuli” that included a looming stimulus, a scrambled stimulus, and a full-field flash. We then analyzed these data in the same way as we did for biological experiments.

The summary of modeling results is shown in **Fig.**, and their comparison to imaging experiments, is given in **Table 2**. In development, the network became selective for looming stimuli, both in terms of the total response (by the end of developme tresponding to looming 99±9% stronger than to flash) and mean selectivity (mean Cohen d of responses = 1.09±0.10). The share of cells selective for looming stimuli also increased, and then saturated at ~98% level, as did the selectivity of the top 10% of most selective cells. 

To see whether in our model the nature of visual stimuli could be reconstructed from network activation, we used a logistic regression on a vector of total cell responses, similar to how it was done for biological data. We then used a different dataset, consisting of equal shares of looming and non-looming stimuli, to assess the accuracy of our model. The prediction power increased in development, and plateaued at the accuracy level of ~95%. This suggests that a retinotopic STDP-driven network can achieve highly reliable looming detection if it is equipped with an output layer, potentially representing motor neurons in the hindbrain, and the weights of connections to this output element are also modified through reinforcement learning.

Unlike in biological experiments, the model was selective for looming stimuli over scrambled stimuli at the full network level, and by the end of the training period, responses to looming stimulus were 43±7% stronger than to scrambled. Mean selectivity of individual cells, defined as a Cohen d for responses to looming compared to scramble, was 0.48±0.07, and 84% of cells were selective for looming stimuli, which is also different from what we observed in biological experiments. The selectivity for looming over flash correlated with selectivity for scrambled stimuli over flash on a cell by cell basis (r=0.17±0.10).

The positioning of selective cells within the network was different in model, compared to biological experiments. While in tadpoles, selective cells tended to be located in the middle of the retinotopic field, in the model they tended to be located on the periphery, and selectivity for looming stimuli positively correlated with the distance from the network center. Similar to tadpoles, however, this correlation disappeared in development (from r ~ 0.75 in a naive network, to r~0.25 in a trained network). Despite the fact that the edges of looming stimuli generally traveled from the center of the retinotopic field to the periphery, there was no correlation between the weight of a cell-to-cell connection and its being facing outward from the network center (r=8e-5), which means that connections were equally likely to face outward and inward. Similarly to biological networks, selective cells tended to be closer to each other (29±3% closer) than expected by chance, yet unlike what we observed in biological networks, this locality of connections was refined in developments.

We then looked at topological and functional correlates of looming selectivity in model networks. Selective cells tended to be more spiky (r=0.34±0.12), with very mild increase in development. They tended not to be a part of a cluster (final correlation with local clustering coefficient r=-0.26±0.12). Unlike in biological networks, in the model selective sells were not special in terms of their Katz centrality (r=0.02±0.13), and they did not tend to receive an unusual number of incoming connections (r=0.00±0.12). Selective cells tended to be connected to each other (weighted assortativity of 0.24±0.07), and on average selectivity did not increase over edges (r=0.00±0.14). In naive networks ~85% of strong edges (top 50% by weight value) tended to lead from less selective sells to more selective ones, but in developed networks this share was reduced to chance value (51±0.03).

The variability of responses to looming stimuli over time, quantified as the number of principal components required to describe 80% of response variability, mildly increased in development, from 28±1 to 37±1. The number of ensembles detected in the network did not change in development, and stayed around 8 to 8.5±0.5, but the share of variance in network responses explained by the involvement of different ensembles, increased from ~35% for a naive network, to ~50% by the end of learning. As in biological results, cells that formed an ensemble were about 10% closer to each other than random cells in the network, and, in developed networks, were 2.2±0.5 times more likely to be connected to each other.

The distribution of degrees in the model was very similar to that in the biological data set: the share of weakly connected cells (weighted in degree < 0.5) plummeted from ~50% in naive networks to 9±2% by the end of training. On the contrary, the share of cells with degrees of 1 and 2 increased from ~40% to 83±2%.  With this changes, the power constant for the degree distribution changed from \gamma ~-1.5 to -1.96±0.03 for both in- and out-degrees, which also qualitatively matched changes in biological experiments.

Finally, we observed that most network measures changed with network development (**Fig**): efficiency and modularity increased, while clustering and cyclicity decreased. In all three cases, the changes were mainly due to changes in weight and degree distribution, as they persisted if calculated on networks randomized with degree-preserving rewiring (**Fig**). The hierarchical flow increased mildly in development, and this was entirely due to structured changes in network topology, as the effect was not present in rewired graphs. A robust increase in modularity may seem to be in contradition with a stable number of ensembles detected in the network, as network modules are expected to form activity ensembles [Triplett 2018]. We assume that the reason for this difference was that both in biological and computational experiments, we tried to identify ensembles in highly structured responses, and not in long recordings of spontaneous activity.

To better assess whether the predictions of the model were confirmed in experiments, we formulated a list of “atomic”, elementary statements about each of the measures were analyzed, looking at whether they changed in development, whether they increased or decreased, and whether they differed from similar measures in a randomized network (**Table 2**). For each of these statements, we then compared the model to the experimental data, and labeled each comparison as either a match (Y), a contradiction (N), or an unclear territory (no label). The main reason for the third, “unclear” option is that in the model, several network properties rapidly changed early in development, but then reached a plateau, or even reversed the direction of the change. With this in mind, when assessing whether the model replicated the phenomena observed in real animals, we would have to first make an assumption about whether stage 46 tadpoles corresponded to a mid-point of network development, or whether both stage 46 and stage 49 tadpoles represent relatively “developed” networks that are kept dynamically fine-tuned. The decrease in the number of reciprocal connections in older networks compared to younger ones suggests that at stage 46 STDP is still actively eliminating connections, placing this stage relatively early in the developmental time scale. On the other hand, we observed no improvement in stimulus identity prediction from neuronal activity, in older tadpoles compared to younger ones, which seems to suggest that both stages fall on the “plateau” of neuronal development, at least as far as the looming selectivity is concerned. In view of this uncertainty, and also honoring the fact that the sample sizes in our imaging experiments were rather low, when the model predicted a change, but this change was not observed in the experiment, we did not count it as an explicit contradiction.

TODO *Describe groups of results: those that were replicated, and those that were not*

## Sensitivity analysis

While a comparison with one faithfully constructed computational model is important, a better approach is to consider a family of models, and see which aspects of a model are critical for the replication of biological results, and which are not essential [Linderman 2017; Pauli 2018]. For example, in our model, how important was to assume that the plasticity mostly happened in response to a learning signal, during actual collisions? Would a presence of structured visual activity, but not necessarily unsuccessful looming avoidance, be enough for looming selectivity development? And is structured sensory flow even necessary for changes in network properties, or would spontaneous activity be enough [Tripplett 2018]? To answer these questions, we repeated our analysis, while gradually simplifying the assumptions of the model. We first let the model develop while exposed to randomized translational stimuli instead of looming stimuli. Then we also destroyed the spatial (but not temporal) structure of retinal inputs, by randomly reshuffling “pixels” in each stimulus. We also let the model develop while exposed to random visual noise. In a different set of experiments, we kept the same structured looming inputs as for the base model, but either relaxed the requirements on synaptic competition, or greatly decreased the amount of intrinsic plasticity present in the system, or replaced STDP with simple symmetrical Hebbian plasticity. The results of these sensitivity experiments are presented in **Figure** and **Table 3**.

# Discussion

TODO

Overall, our analysis of connectivity in the tadpole brain showed that tectal networks are highly non-random, with almost all network parameters (efficiency, modularity, clustering, hierarchy, and cyclicity) significantly different from what would be expected in a similar randomized network. At the same time, except for changes in degree distribution and modularity, we did not detect strong differences between tectal networks in younger and older tadpoles. This suggests that while stage 45-46 tadpoles differ from older, stage 48-49 tadpoles in terms of synaptic and intrinsic properties of neurons in the tectum [Ciar], the patterns of tectal connectivity may be relatively settled by stage 46. This, in turn, suggests that weaker collision avoidance in stage 45-46 tadpoles [Dong] may be due to maturation of sensorimotor circuits in the hindbrain, which we didn’t assess in this study.

Another potential explanation for this negative result, is a relatively low power of our study. Based on the results of imaging studies [XXX], we can estimate that at stage 49, one side of a tadpole tectum contains about 10-15 thousand neurons, as the tectum is about 40 cells across, and packed 6-10 cells deep in its thickest part, while tapering towards the edges. On the other hand, in our study we reconstructed connectivity within the top layer of cells, in a field of about 12 by 12 neurons, which suggests that our reconstructions covered about 1% of the full tectal network. With a coverage so sparse, we could hope to only detect the strongest and most robust changes in network organization. This is further complicated by the fact that younger tadpoles have fewer neurons in their brains, and so in these experiments, the subset of cells we recorded constituted a larger share of total neurons.

The graphs we reconstructed differed from random graphs in several important, expected, and interpretable ways, which supports the reality of the effects we describe. TODO

**For why scramble was significantly stronger than looming in some experiments, but weaker in others**. Because spatial rearrangement was unique for each experiment (different across experiments, but constant within each experiment), it is possible that some of our rearrangements produced an objectively “worse” stimuli (as we expected), but some produced a supernormal stimulus, or a sort of a visual illusion that looked more salient than a collision. A good way to test this hypothesis in the future would be to rearrange the stimulus in several different ways within each experiment, and see whether the preference for either looming or scrambled stimulation at the level of tectal network as a whole is indeed tadpole-dependent, or stimulus-dependent. Regardless of the answer to this question, however, we can safely state that at the very least scrambled looming stimuli are about as salient for stage 46-49 Xenopus tadpoles as looming stimuli. It also suggests that the neural representation of the collision trajectory, and the calculation of the most effective escape route that was observed in our prior behavioral experiments [Khakhalin 2014], are more likely to happen at the interface between the tectum and the hindbrain motor nuclei, or even in these nuclei, rather than in the tectum.

Mention the STDP controversy: does it even exist? If it happens to work nicely in our model, does it provide some sort of evidence that at the very least it may be useful for development?

Our experiments were not ideal for ensemble detection, as we have a strong shared input that drives network activity and most probably differentially activates different functional ensembles in the network. During spontaneous activity, each subnetwork, such as the collision detection subnetwork, would be activated relatively independently from the rest of the tectum, and then the activity would propagate within this subnetwork (as it is more tightly connected), enabling robust ensemble detection [Avitan 2017; Triplett 2018]. We, on the other hand, reliably activated entire network with strong visual stimuli, and so neuronal ensembles could only manifest themselves as slight amplification of stimulus variability on a trial-by-trial basis.

In the modeling part of our study, we looked at the development of a looming-detecting network that only was rewarded, and thus rewired, after direct frontal collisions, but a real swimming animal experiences different types of looming stimuli that come from different directions, and have different degrees of a translational component. The final result of an unsuccessful collision avoidance would also be somewhat different for a real animal: the rapid dimming of optic receptors would be experienced by different parts of the retina; different subsets of the lateral line and mechanosensory receptors would get activated. Therefore, a full brain may harbor several similar overlapping subnetwork, each selective for stimuli of different geometry, getting reinforced by different learning signals, and projecting to different subsets of the motor network.

We show that STDP leads to increase in network modularity, similar to how it was shown to happen for classic Hebbian plasticity [Damicelli 2018; Triplett 2018]. TODO

# Methods
## Statistics and reporting

Unless stated otherwise, all values are reported as mean ± standard deviation. For most common tests, the type of a test is indicated by the subscript for its reported p-value: p_t for a two-sample t-test with two tails and unequal variances; p_t1 for a one-sample two-tail t-test; p_r for a Pearson correlation test; p_binom for a one-tail binomial test, and p_F for the analysis of variance (ANOVA, ANCOVA, multilinear regression).

Note also that in this paper we consistely describe adjacency matrices as they are used in computational neuroscience, where w_{ji} is a weight of an edge coming from node i to node j, which is different from how adjacency matrices are used in graph theory, where A_{ji} would typically mean an edge from node j to node i.

## Experiments

Our experiments followed the procedure previously described in [Xu 2011; Truszkowski 2017], with visual stimulation described in [Khakhalin 2014]. All procedures were in accordance with Brown University IACUC protocols. Except noted otherwise, all chemicals were purchased from Sigma. Tadpoles were kept in Steinberg’s solution, on a 12/12 light cycle, at 18 \deg C for 10-20 days, until they reached Nieuwkoop-Faber developmental stages 45-46 or 48-49. In each experiment, we anesthetized a tadpole with 0.02% tricainemethane sulfonate (MS-222) solution for 5 minutes, then paralyzed it by immersion in 20 mM solution of tubacurarine for 5 minutes, and pinned it down to a carved Sylgard block within the recording chamber, filled with artificial cerebro-spinal fluid solution (ACSF: 115 mM NaCl, 4 mM KCl, 5 mM HEPES, 10 µM glycine, 10 mM glucose). The optic tectum was exposed, and ventricular membrane was removed on one side of the tectum. Tadpoles were pinned tilted, at an angle of 10-20 \deg, to keep the exposed tectal surface flat for imaging. We then surrounded the tadpole with a small circular enclosure 15 mm in diameter, made of a thicker part of a standard plastic transfer pipette, to achieve higher concentration of Ca-sensitive dye in the solution. We dissolved 50 ug of Oregon Green Bapta 1 solution (OGB1 #06807, Molecular Probes, Waltham, MA) in 30 ul of medium consisting of 4% F-127 detergent in 96% DMSO by weight; agitated this solution in a sonicator for 15 minutes, then added 30 µl of ACSF to the vial, and sonicated for 10 minutes more. The solution was then transferred to the chamber, mixed with 4 ml of ACSF to the final concentration of 10 uM, and the chamber was placed in the dark for 1 hour. After staining, the circular enclosure was removed; the preparation was washed with 10 ml of ACSF 3 times; the chamber was filled with 10 ml of fresh ACSF, and transferred under the scope. 

Visual stimulation was provided with a previously described setup [Khakhalin 2014], consisting of an LCD screen (Kopin Corporation, Taunton, MA, USA) illuminated by a blue LED (LXHL-LB3C, 490 nm; Lumileds Lighting, USA), with the image projected to an optic multifiber (600 um, Fujikura Ltd, Tokyo, Japan). The other end of the fiber was brought to the left eye of the tadpole, and placed 400 um away from the lens, and on the axis of the eye, to have the image projected to the center of the retina. The stimulation sequence consisted of three stimuli: looming stimulus (in which a circle appeared in the center of the field, its radius growing linearly from 0 to full-field within 1 second), full-field flash, and spatially “scrambled” stimulus. For the scrambled stimulus, we divided the field of view into a grid of 17 by 17 squares and randomly reassigned these squares within the image. The result was a stimulus that was identical to looming stimulus in terms of its total brightness at every time step, and presented fragments of a moving edge locally (within every square in a reshuffling grid), but lacked large-scale spatial organization. The permutation of squares within the grid was randomized for each experiment, but consistent within all trials within an experiment. The stimuli were delivered every 20 s, in a sequence “looming, flash, scrambled”, usually for the total of 60 or 72 stimuli. The stimuli were generated in Matlab (Mathworks), using Psychtoolbox [Kleiner 2007]. Imaging excitation light was turned on 1 s before the onset of the visual stimulus, which was shown not to interfere with visual stimulation [Xu 2011], and kept on for 5 s.

The tectum was imaged using a Nikon Eclipse FN1 microscope with a 60x water-immersion objective and an ANDOR 860 EM-CCD camera. NIS-elements software (Nikon) was used to record the activity, with binning of 8x8 pixels per bin, resulting in a 130x130 image covering the field of view of 1130 um. The data was acquired with 10 ms auto-exposure, which led to actual frame rate of 84 frames per second (11.9 ms per frame). For each preparation, we used a focal plane that produced images of as many cells as possible, which usually meant a plane focused “in-between” the topmost and bottom-most cells within the field of view. To keep the signal to noise ratio consistent throughout the experiment despite the ongoing bleaching of the Ca sensor, we started with relatively weak illumination (with neutral density filter ND4 engaged) and no signal amplification by the camera (EM gain of 0). We then increased the EM gain level gradually after every 12 stimuli, to keep the signal level approximately constant. Once EM gain setting reached the value of “7”, we increased illumination strength by disengaging one of the density filters, reduced EM gain back to 0, and repeated the process. 

Videos were processed offline; circular regions of interest of equal size (21 bins per region) were manually positioned over neurons with well defined, highly variable Ca responses. The average fluorescence within each region of interest was quantified, and exported to Matlab. We processed fluorescence traces with a non-negative deconvolution algorithm [Vogelstein], and used its output without thresholding, interpreting it as an estimation of both timing and number of possible spikes produced by each cell [refs?]. We chose this approach, as depending on the overlap each cell body had with the focal plane, and the amount of dye sequestered, different neurons had very different signal-to-noise ratios, which complicated the matter of finding a single threshold. This decision also shaped all further steps of analysis, as in our dataset both poorly resolved cells with low spiking activity were represented not by spike traces that were mostly silent, but by traces that approached a maximum entropy, uniform distribution of estimator values. For the purposes of deconvolution, in each recording the reference cell was selected automatically, as the cell with 5th highest amplitude fluorescence response. 

In this series of experiments, we did not attempt to match inferred spike trains to the “ground-truth” electrophysiological recording from a reference cell, as overall validity of this calcium imaging protocol was justified previously [Xu, Torrey]. We also did not perform background subtraction [refs], as most effect of background fluorescence were expected to be cancelled out at later stages of analysis. The main risk of not subtracting the background is that unsubtracted traces may contain a superposition of axonal spiking and synaptic activation in the neuropil. In our experiments, the bulk of neuropil activation was expected to be similar in every trial, as stimuli presented to the tadpole were always the same. Moreover, our video acquisition was by design highly sensitive to fluorescence sources lying within the focal plane, which means that the neuropil signal was both greatly attenuated, and spatially averaged. As deconvolution operation is close to linear, and we did not perform spike thresholding, any shared neuropil signal would be deconvolved, “hidden” in inferred spike-trains, and later cancelled out during trial-reshuffling (see below). Finally, we did not address motion artifacts, as in our preparation they were synchronous in all cells (manifested as parallel displacement of signal sources from fixed ROIs), and therefore only introduced a fixed bias to all TE estimations.

## Analysis

**Basic analysis**
*How response amplitudes were measured (cumulative on a range XXX to XXX) TODO*

As a measure of stimulus selectivity for each cell, and in some cases for the selectivity of total network response, we used Cohen’s d effect size for the difference between responses to looming and flash, or looming and scrambled stimuli:

$$d = (m_L-m_F)/ \sqrt{ \big((n_L-1) s^2_L + (n_F-1) s^2_F)/(n_L + n_F - 2)} = (m_L-m_F)/\sqrt{\big(s^2_L+s^2_F\big)/2}$$
in case of equal sample sizes n_L=n_F=n; here m_L and m_F are mean responses to looming and flash stimuli respectively, and s_L , s_F are standard deviations for both groups. To verify that Cohen d captured the nature of selectivity to a certain stimulus type, we also quantified selectivity as McFadden’s pseudo-$$r^2$$, by fitting total spiking response of each cell to stimulus identity, and assessing the quality of this fit (see Methods). Across all cells recorded in all experiments, both measures correlated (r=0.66±XXX), and the results on pseudo-$$r^2$$ were qualitatively similar to those on Cohen’s d.

To find the retinotopy center, we concatenated all responses of every cell to looming stimuli into one long vector, ran a principal component analysis on these vectors, then rotated two first components using promax rotation, and made sure that the 1st component c^1 is the one with shorter latency, and that it is positive, flipping the components if necessary. We then ran a non-linear optimization, looking for a pair of coordinates for the “retinotopy center” (x,y) that would maximize the absolute value of correlation between distances of each cell to this center and the prominence of the short-latency component in this cell:

$$r = \text{cor}\big(\sqrt{(x_i-x)^2+(y_i-y)^2}\ ,\ c^1_i/(c^1_i + c^2_i)\big)$$

For response latency calculations, we looked at each response y(t), and found the position of its maximum (x_M, y_M). We then used the least squares fit with non-linear solved to approximate the segment between the beginning of the response and x_M with a piecewise linear function:

$$f(x) = \begin{cases} 0 & \text{for} \; 0\leqslant x<x_L \\ a(x-x_L) & \text{for} \; x_L\leqslant x < x_M\end{cases}$$
optimizing for x_L and a, where x_L is the response latency, and a is an amplitude-like parameter we did not use for subsequent analysis. This approach worked well for isolated responses with good signal to noise ratio, but got increasingly noisy with weak signals. To quantify the retinotopy, we therefore used the results of factor analysis, and only referred to response latencies for verification.

**Ensemble analysis**
To find ensembles of cells that tended to be co-active together, we used a modified spectral clustering procedure [Ng 2002] and the definitinon of spectral modularity [Newman 2006], generalized to weighted oriented graphs. First, for each stimulus type, for each cell i, and separately for each experimental trial k, we unbiased and normalized each activity response a^k_i(t), by subtracting its mean, and dividing the result over standard deviation:

a^k_i(t)’ = (a^k_i(t)-b^k_i)/\sigma^k_i}

where b^k_i = \sum_{t=1}^T{a^k_i(t)} and sigma^k_i = \fraq{1}{T-1}\sum_{t=1}^T{(a^k_i(t) - b^k_i))} .

Then, for each cell, we calculated the average response across all trials of the same type:  \over{a_i(t)} = \fraq{1}{n}\sum_{k=1}^n{a^k_i(t)}, and subtracted these average responses from each trial, which resulted in a vector of a trial-by-trial deviations from the average response:

a^k_i(t)’’ = a^k_i(t)’ - \over{a_i(t)}

We then used these vectors of deviations from the mean, concatenated across all trials, to calculate a cross-correlation matrix, to see which cells tended to be unusually active or unusually inactive together:

c_{ij} = \text{corr}(a’’_i(k,t),a’’_j(k,t))

We calculated adjusted correlations c_{ij} separately for each of three types of stimuli (flash, scramble, and looming), and averaged these three estimations c_{ij}^s, to arrive at a, hopefully, less noisy estimation of adjusted cross-correlation. We then removed negative correlations, replacing them with zeroes.

c’_{ij} = \text{max}(0, \frac{1}{3}\sum_{s}{c_{ij}^s}

We then roughly followed the spectral clustering approach by [Ng 2002], with some adjustments that seemed appropriate for ensemble detection.  We first transformed our correlation matrix c_{ij} into a matrix of pairwise Euclidean distances:

d_{ij} = 2(1-c_{ij}), 

and then to affinity matrix:

A_{ij} = exp(-d_{ij}/\sigma) 

where \sigma is a free parameter that we set at 10000. We then calculated a diagonal degree matrix D such that D_{ij} = 0 for i \neq j , and D_{ii} = \sum_k{A_ik} otherwise; used it to build a Laplacian matrix L, such as:

L_{ij} = A_{ij}/\sqrt{D_ii*D_jj}

, and found eigenvectors x_1 .. x_n of matrix L. Then we selected a number of ensembles to find k, going through all values from 1 (no ensembles) and up to the number of cells (each cell as a separate ensemble). For each k, we found first k largest eigenvectors of L, stacked them in columns, and renormalized each row of this matrix to give it unit length:

U_{lm} = x_{lm}/\sqrt{\sum_{z=1}^{k}{x_{lz}^2}}

where x_{lm} is an m-th element of l-th eigenvector of L. We then used k-means clustering on rows of U as points in R^k, looking for k clusters.

Once rows of U (and so cells in the original data) were assigned to k clusters, we calculated spectral modularity of this division on the original matrix w_{ij}, using a weighted directed modification of classic formula from [Newman 2006]:

Q_k = \frac{1}{4m}\sum_{ij}{\delta_{ij}(w_{ij}-\frac{d^{out}_i d^{in}_j}{2m}})

Here d^{out}_i and d^{in}_j are weighted out- and in-degrees for nodes i and j respectively: 
d^{out}_i = \sum_k{w_ik} , and d^{in}_j = \sum_k{w_kj} ; m is the total number of edges involved: m = \sum_{ij}{w_{ij}}/2 , and \delta_{ij} is a signal matrix with \delta_{ij}=1 for nodes i and j that belong to the same cluster, and \delta_{ij} = 0 otherwise. We then found the number of clusters K that, after spectral clustering, produced highest modularity Q_K across all Q_k, and used K as an estimation of the number of ensembles in the network, and corresponding cluster allocation - as the allocation of cells to these ensembles.

**Network reconstruction**
For network reconstruction, we used a modified Transfer Entropy (TE) calculation, adapted from [Stetter 2012, Gerhard 2013, Gourevitch 2007]. Mathematically, fast Ca imaging recordings, as used in this study, provides a middle ground between commonly used, slower Ca imaging data and multielectrode recordings. In most Ca imaging recordings, the frame acquisition time (~100 ms) is an order of magnitude longer than the transmission time between neurons (~ 2 ms), which biases analysis towards co-activation analysis. In our data, the high rate of acquisition (12 ms per frame) was very close to typical cell-to-cell activation transmission time in the tectum, so we restricted our analysis to interactions between the activity of each cell at a frame i and their activity at the next frame i+1, ignoring both longer (multiframe), and same-frame interactions.

For each cell, we took its inferred activity train, and binned it at 3 levels, classifying every frame as either a frame with high, medium, or low activity. For each cell, we used 1/3 and 2/3 quantiles of its inferred activity train values as levels thresholds, so that all three types of frames were equally frequent, as this optimized the entropy of representation, while retaining low binning count. Then for each pair of neurons i and j we calculated the probability p(k_j^1,k_j^0,k_i^0), which showed the conditional probability of neuron j being in state k_j^1 (1 to 3) at moment t, if this neuron was in a state k_j^0 at the previous frame t-1, while input neuron i was in state k_i^0 at frame t-1. From this set of probabilities, we also calculated conditional probabilities of P(k_j^1 \mid k_j^0), and finally calculated the total transfer entropy as

T_{ij} = \sum_{lmn=1}^3{P(k_j^1=l,k_j^0=m,k_i^0=n)}\cdot \log(\frac{P(k_j^1=l \mid k_j^0=m,k_i^0=n)}{P(k_j^1=l \mid k_j^0=m)})

$$T_{ij} = \sum_{lmn=1}^3{P(k_j^1=l,k_j^0=m,k_i^0=n)}\cdot \log\left(\frac{P(k_j^1=l \, \mid \, k_j^0=m, \, k_i^0=n)}{P(k_j^1=l \, \mid \, k_j^0=m)}\right)$$

In our project, common drive (visual input from the retina) presents a particular problem. If detection of looming stimuli happens mainly through activation of selected synfire chains, the pattern of this activation would be necessarily synchronized with the causal transfer of excitation from one node to another. Because of that, it cannot be eliminated by methods that rely on the comparison of delays [Wibral 2012, Wollstadt 2014]. Instead, we eliminated the effects of common drive by reshuffling our data, and pairing activation history of each cell with activation history of other cells for reshuffled, unmatching trials recorded in response to same stimulus type. For each experiment, we calculated 1000 randomly reshuffled transfer entropy estimations, and then subtracted the average of these reshuffled TE estimations from our TE estimation, arriving at the value of adjusted TE [Gourevich 2007]:

T’_ij = T_ij - T^shuffled_ij

This approach is similar to the idea of analyzing subtle variations in activation from one response to another, as opposed to the analysis of activation traces themselves. As the stimuli we presented were same in every trial, the progression of the common drive over time was shared across all trials. If a connection between cells i and j was suggested by the analysis of reshuffled data, these cells were clearly sequentially driven by a common input, and not by a true causal connection between them.

For each TE estimation, we also calculated a corresponding p-value, to quantify whether the observed TE was significantly different from the set of TE estimations obtained on surrogate data, corresponding to H0 of fully shared drive, and no causal connections. With the computational power available, we could only afford to generate 1000 surrogate reshuffled networks for every TE calculation, which made it impossible to use the false discovery rate correction on our data, as it is recommended for fMRI-based studies of large-scale brain connectivity [Vicente 2011; Lindner 2011]. With ~10^2 neurons and 10^4 connections the smallest possible non-zero p-value of 0.001, corresponding to finding a more extreme TE value in one out of 1000 surrogate experiments, was already larger than the Benjamini-Hochberg threshold of \frac{k}{m}\alpha=5e-6. With a permissive threshold of \alpha=0.01, each subset of data (recordings of responses to collisions, flashes, and scrambled stimuli), when taken separately, suggested the existence of 2% to 69% of all possible directed edges in the graph, depending on the experiment (median of 8%). The share of edges that were independently discovered in all three types of experiments (median value of 0.1% of all possible edges) was on average 1.8 times larger than one would expect in case of spurious and independent discovery (signrank p=7e-7), suggesting that the three subsets of data, originating from responses to three different stimuli, can be considered replications for the purposes of edge discovery. At the same time, when we tried to restrict our analysis only to edges that were fully replicated in all 3 sets with \alpha < 0.05, we ended up with the median graph size of only 18 edges (6 edges in the largest connected component). The overwhelming majority (18 out of 19) of datasets with fewer than 10 reconstructed edges were recorded in the early set of experiments, when our dye transfer from DMSO to ACSF was still imperfect. As during data acquisition, we alternated between stage 46 and 49 tadpoles, the set of “weak” experiments was not biased, and consisted of 10 younger, and 9 older tadpoles. We decided to exclude these 19 “weak” experiments, and believe that restricting all analysis to remaining 30 experiments did not bias the study.

One of the unique challenges of imaging protocols used in this study was the variable quality of staining achieved with the BAPTA-conjugated dye. As staining procedure involved a detergent, and called for high concentrations of dye, the most successful preparations were those that received the highest possible concentration of dye that did not yet kill the cells. Several preparations however either fell a little short of this target, and had a higher than optimal level of noise, or got overexposed, and had strong fluorescence, but weak responses to stimulation. The variable amount of uncertainty in edge detection from one experiment to another greatly complicated our network analysis.

To make the edge inference more robust, for 30 experiments included in further analysis, we relaxed our criteria on edge discovery, while still giving preference to edges discovered independently in more than one subset of responses. To do so, we included in our reconstruction only edges with geometric mean of p-values below significance threshold: \prod{p_k}<\alpha^3 , where p_k are p-values for each of three subsets of data (responses to flash, crash, and scrambled stimuli). Because of variation in staining quality and focal plane alignment, we could not use a fixed significance threshold \alpha for all experiments, and instead followed an approach common in analysis of noisy networks, setting the average node degree (the ratio of network edges to network nodes, for directed graphs <k>=E/N) to an arbitrary reasonable value [Stetter 2012]. For this study, we picked a value of 1.0 (number of edges equal to the number of nodes), which lead to 128±41 edges in each experiment on average (0.9% of all possible edges); 50\pm21 weakly connected components, and 74±30 nodes in the largest connected component. The comparison of network properties (**Fig**) did not change qualitatively in a broad range of assumed average degrees (from ~ 0.5 to 1.5), but observed effects became weaker and regressed to random effects outside of this range.

The TE approach did not distinguish between positive and negative influence of one neuron on another, so our reconstructed edges could include a mix of excitatory and inhibitory connections. To estimate the share of putative inhibitory connections, we calculated pairwise correlations between activities of individual neurons, compensating for the effect of shared inputs through trial reshuffling (similar to how it was done for TE), and looked at the sign of these correlations for pairs of neurons with TE>0. We found that 3\pm7% of detected connections seemed inhibitory or inactivating, with no difference between developmental stages (d=0.55, p_t=0.1). According to our current understanding on the tectal architecture, deepest principal tectal neurons are not expected to be inhibitory [REF?], and the share of negative correlations tended to be lower in experiments with better signal-to-noise ratio, suggesting that at least part of observed inactivating connections may be false discoveries. 

To analyse **degree distributions**, we **TODO**
*How did we round them? Because this is supposedly a weighted graph?*

**Network analysis**
We reviewed several lists of statistical and topological metrics of weighted undirected graphs [refs], and selected a sufficiently diverse set of measures that described different aspects of graphs, including average connectivity, unevenness of density, and global structure, and did not change too strongly with the inclusion or exclusion of individual weakly connected nodes. This last point is important, as some topological properties of a graph, such as its cycle order, or the presence of broadly defined “small world” properties, can be notoriously sensitive to the inclusion of only a few weak long-ranged connections [refs]. We wanted to make sure that the measures chosen for network topology description would not change catastrophically from one experiment, or one animal group, to another because of small variations in the level of noise, or a slightly more generous selection of regions of interests during Ca imaging quantification. We ended up with the following list of network metrics:

**Global network efficiency** [refs] was calculated using a function from the Brain Connectivity Toolbox [Rubinov Sporns] on reciprocals for graph weights R_{ij} = 1/w_{ij}, and was defined as:

E = \frac{1}{n} \sum_{i \neq j}^n{\frac{d_{ij}}{n-1}} where d_{ij} is the length of the shortest path P_{ij} connecting nodes i and j: d_{ij} = \sum_{kl \in P_{ij}}{R_{kl}}

**Clustering coefficient** [Fagiolo 2007] was calculated using the Brain Connectivity Toolbox, a function for the weighted directed variety of the clustering coefficient [Rubinov Sporns] according to the formula:

C = \frac{1}{n} \sum_i{\frac {t_i}{(k^o_i+k^i_i)(k^o_i+k^i_i-1)-2\sum_j{w_{ij}w_{ji}}}}

where k^o_i and k^i_i are out- and in-degrees of node i respectively, and t_i is the weighted number of directed triangles that include node i: 

t_i = \sum_{j \neq i}{\sum_{k \neq i,j}{w^{1/3}_{ij}w^{1/3}_{jk}w^{1/3}_{ki}}}

For **spectral modularity**  we used a TODO

Our custom definition of **hierarchical flow** was inspired by [ref], but based on the modified Katz centrality [Katz 1953, Fletcher 2017]. To calculate Katz centrality, we assumed that on average, each node i collected a flow of incoming signals through all edges w_{ij} leading to this node. The activation arriving through edge w_{ij} was proportional to total activation z_j of node j, the edge weight w_{ij}, a normalization coefficient equal to 1/\text{max}*{*kl}(w{kl}), and a damping factor of d=0.9, and also received a small amount of constant activation (1-d)=0.1. The total activation of each node was therefore defined as:

z_i = (1-d) + \fraq{d}{max(w_{kl})} \sum_{j \neq i}{a_j w_ij}

Each node then further redirected this activation to other nodes. This definition is very close to that of pagerank centrality [Page 1999], with a minor difference that the weights are not normalized to the value of total outgoing weights for each node: that is, we work with raw weights of w_{ij} rather than w_{ij}/\sum_k{w_kj}.  It means that a node with many outputs has a strong influence over network activation, nodes with weak outgoing edges act almost as dead-ends. Similar to a standard pagerank algorithm, we found solution the stable solution of this problem iteratively, by initializing the network with equal values of centrality, and then running the equation above 100 times or until convergence. Once the distribution of centralities z_i stabilized, we used the difference between the maximal Katz centrality and mean centrality across all nodes as a measure of hierarchical flow in the network [ref]:

h = max_i(z_i) - mean_i(z_i)

Custom definition of **cyclicity**, or the prevalence of cycles in the graph. TODO

To check whether network values described above were different from values expected on a random graph, we performed a two-stage graph randomization. First, we randomized each graph using a variety **degree-preserving reshuffling** [Maslov 2002]. To do so, for a network with N_E edges we picked 3\cdot N_E random pairs of nodes (nodes i, j, k, and l) that had strong connections from i to j, and from k to l, but weak connections from i to l, and from j to k (we required w_{ji}>w_{li} and w_{lk}>w_{jk}). We also required all four nodes be different (i \neq j \neq k \neq l). Then we cross-wired them, thus gradually randomizing network topology:

$$\begin{cases} w_{ji} \leftarrow w_{li} \\ w_{li} \leftarrow w_{ji} \\ w_{lk} \leftarrow w_{jk} \\ w_{jk} \leftarrow w_{lk} \end{cases}$$

This approach to degree-preserving randomization is slightly different from the original formulation by [Maslov 2002] in two ways. First, we explicitly don’t allow loops (self-edges) by requiring all four nodes be different. Second, we allow nodes i and k, as well as l and j to be connected before the rewiring, and just swap corresponding edge weights, which seems to be a necessary adjustment for directed weighted graphs. It also means that for strictly speaking, for a weighted graph, our randomization only preserves out-degrees, but not in-degrees. Because of the requirement that w_{ji}>w_{li} and w_{lk}>w_{jk}, for a binary directed graph our algorithm preserves in-degrees strictly, as it becomes identical too [Maslov 2002], while for nearly-binary graphs (either bimodal or sparse), it will tend to preserve in-degrees on average. 

Then, at the next step, we completely randomized connections by randomly swapping weights of edges w_{ji} and w_{lk}, including zero weighs, which disrupted both in- and out-degree distributions, and led to a random Erdos graph with the same distribution of weight values w_{ij} as in the original graph. We then compared original network measures (on the actually observed graph) to those calculated on degree-preserving rewired graphs (averaged across 20-50 rewirings), and to those calculated on fully rewired graphs (averaged across same number of rewirings). This allowed us too differentiate between the effects of graph structure, and non-random degree distribution.

We also tested whether the connectivity and position of selective cells within the graph is in any way peculiar, by calculating Pearson correlations between cell selectivity and several different graph centrality measurements. We used the following list of centrality measures: **TODO**

To study the co-distribution of node properties (cell selectivity) within the connectivity graph, we used **weighted assortativity** as a measure of non-random association of selective cells into subnetworks. The formula for a mixing coefficient in a weighted directed network is given in [Farine 2014], based on the logic from [Newman 2003] and [Leung 2007]. The original formula from [Newman 2003] for an unweighted undirected graph defines a mixing coefficient as a Pearson correlation coefficient between properties of nodes connected by edges, taken over all edges in the graph:

r=\text{cor}_{ij: a_{ij}=1}(x_i,x_j)

leading to the following expression:

r = \frac{\frac{1}{E} \sum{x_i x_j} - [\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2} {\frac{1}{E} \sum{(x_i^2+x_j^2)}-[\frac{1}{E} \sum{\frac{1}{2}(x_i+x_j)}]^2}

where sums are taken over all connected edges ij: a_{ij}=1, and E is the total number of edges.

For a weighted graph an equivalent measure can be introduced by replacing summation over edges to summation over all possible pairs of nodes $$ij$$, with weights $$w_{ij}$$ introduced in each sum. The resulting expression can be rewritten in several different ways [Newman 2003; Leung 2007; Farine 2014; Teller 2014], with several alternative bulky expressions ultimately describing a weighted correlation calculated across all connected directed edges ij:

r=\text{cor}(x_i,x_j,w_{ij})

where weighted correlation cor(a,b,w) is introduced through weighted covariances:
cor(a,b,w) = \frac{cov(a,b,w)}{\sqrt{cov(a,a,w) \cdot cov(b,b,w)}}
that in turn are defined as:
cov(a,b,w) = \frac{\sum_i{w_i \cdot (a_i-\bar{a})(b_i-\bar{b})}}{\sum_i{w_i}}

with \bar{a} and \bar{b} representing weighted mean values:
\bar{a}=\sum_i{w_i a_i}/\sum_i{w_i}

Note that this formula differs slightly from the one used in the Brain Connectivity Toolbox [Rubniov].

## Developmental Model

The model consisted of n=81 cells, arranged in a 9x9 grid. The model operated in discrete time, and was run for 500 epochs, 25 time steps each, or for T = 12500 time steps total. Each cell was characterized by three values: its current activity s_i(t) that represented its instantaneous spiking rate; spiking threshold h_i(t) that slowly changed over time, and a constant \hat{s_i} that described the target spiking rate for each cell. The target spiking rates were randomly assigned at the beginning of each simulation, and were distributed normally around 5/n with a standard deviation of 1/n, 

\hat{r_i} = \mathcal{N}(\frac{5}{n},\frac{1}{n})

which means that when these target spiking rates were matched, on average, at any time step, 5 out of 81 cells would be spiking. The spiking thresholds h_i were initialized at the beginning of each simulation with a value

h_i(0) = 1/(n \hat{s_i}) + \mathcal{N}(0,0.1)

where \mathcal{N}(0,0.1) represents a random value, normally distributed around 0 with a standard deviation of 0.1 .

Cells were connected to each other with “synapses” of different strengths, represented by a weight matrix W, with weight 0 \leqslant w_{ji} \leqslant 1 leading from cell i to cell j. At the beginning of each simulation the weights were assigned random values, uniformly distributed between 0 and 1, except for self-connections (loops, w_{ii}) that were set to 0.

At each time step we first calculated the raw activation of all neurons A:

A = WS + B

where W is the connectivity matrix, S is the vector of instantaneous spiking rates s_i , and B is the sensory input (see below). For one cell, we have:

$$a_i(t+1) = \sum_j{w_{ij}s_j(t)} + b_i(t)$$

These raw activation values were then adjusted down, by a formula reminiscent of global feedback inhibition, which helped to avoid run-away activation:

$$a'_i(t+1) = \begin{cases} a_i(t+1), & \text{if } \sum_j{s_j(t)} \leq d \\ a_i(t+1)\Big/ \Big(1 + \big(\sum_j{s_j(t)} - d\big) \cdot \exp(- t/\tau_e)\Big), & \text{otherwise.} \end{cases}$$

Here a’_i(t) is the final, adjusted value of activation for every cell; \sum_j{s_j(t)} is the total sum of all cell activities at the previous time step; d is a constant that sets the level of total activity at which inhibition “turns on”, and that in our case was set to the size of the grid of cells d=9. The exponent \exp(-t/\tau_e) serves as an “easing” function that gradually “eases” the network from inhibition-dominated mode of operation to “free” operation, with a time constant \tau_e=T/7. This “easing” formula was a practical compromise that greatly sped up our computational experiments, as it dampened network activity early on, when network was still close to randomly connected, and so prone to seizure-like activity, but allowed the simulation run on its own later in development. 

The activity of each neuron s_i(t) was then calculated from its total activation a’_i(t):

$$s_i(t) = g_i\big(a'_i(t)\big)$$

using a logistic activation function:
$$g_i(a) = 1/\Big(1+\exp\big(c(h_i(t)-a)\big)\Big)$$

where c is a steepness parameter, set at c=20, and h_i(t) is the current spiking threshold of cell i. At the beginning of each simulation, spiking thresholds h_i(0) were set to random values, uniformly distributed in a narrow band between 1/(n \hat{s_i}) and 1/(n \hat{s_i}_i)+0.1 . During the simulation, the thresholds h_i(t) were updated at each time step, to model the effect of **intrinsic homeostatic plasticity**. For this purpose, for each cell, we kept track of its running average spiking rate \bar{s_i}(t), and updated both average spiking rates and spiking thresholds h_i(t) by the following formulas:

$$\bar{s_i}(t+1) = (1-\kappa)\bar{s_i}(t) + \kappa s_i(t)$$

$$h_i(t+1) = h_i(t) - r_h(\hat{s_i} - \bar{s_i}(t))$$

where \kappa=0.05 is a constant that controls the rate of averaging, and r_h=0.1 is the rate at which spiking thresholds h_i were allowed to adjust, to bring the discrepancy between the target spiking rate \hat{s_i} and running average spiking rate \bar{s_i}(t) to zero.

Once spiking of each neuron at the new time step s_i(t) was calculated, we performed the **spike-time dependent plasticity** (STDP) step, and adjusted synaptic weights w_{ji} linking neurons in the network. The intuition behind STDP in discrete time can be described by the following system, with options 1 and 2 not mutually exclusive:

$$w_{ji}(t+1) = \begin{cases} w_{ji}(t)+\epsilon, & \text{if } s_i(t)\neq 0 \text{ and } s_j(t+1)\neq 0 \\  w_{ji}(t)-\epsilon, & \text{if } s_i(t)\neq 0 \text{ and } s_j(t)\neq 0 \\ w_{ji}(t) & \text{if }s_i(t)=0\end{cases}$$

As in our model neuronal activity s_i(t) was continuous, the non-exclusive system above can be represented by the formula:

$$w_{ji}(t+1) = w_{ji}(t) + r_w \big(s_i(t)w_{ji}(t)s_j(t+1) - s_i(t)w_{ji}(t)s_j(t)\big)$$

or

$$w_{ji}(t+1) = w_{ji}(t)\cdot\Big(1+r_w\big(s_j(t+1)-s_j(t)\big)s_i(t)\Big)$$

where r_w is a constant that controls the level of synaptic plasticity; in our case r_w = 0.25 .

At the last step, we introduced **synaptic competition**, to make sure that **TODO**

As learning times used in this model were 2 (?) orders of magnitude shorter than those experienced by a tadpole (and ~3 for mammals), we used a special procedure to "ease" the network  into its final state…**TODO**

**TODO** *Explain how we prepared spiking threshold for each cell, and that this is actually a meaningful procedure that can affect the results.*

This operation was necessary, and could have affected the model in not trivial ways: as neurons in STDP-dominated networks seem to be arranged ensembles and synfire chains [XXX], and so are activated together, intrinsic plasticity in these networks is not just adjusting individual neuron excitability. Instead, it is likely to act as a major force, regulating relative sensitivity of a network to different sensory patterns, assuming that these patterns are detected by different sub-networks. For example, in our model, we found that high levels of spontaneous activity during excitability pre-tuning **TODO**

This effect was so prominent in the model, that we hypothesize that it may be relevant in the biological tectum as well. To maintain the network of synaptic connections, each ensemble of synfire chains has to be regularly activated, yet the more active it is, the less excitable the neurons become, which means that they are less likely to win during competition with other ensembles during stimulus detection. The dynamics of plasticity in the brain would therefore pose a meta-balancing problem, as if intrinsic plasticity is too flexible, the network that detects unusual, threatening stimuli, will get spontaneously activated in the absence of these stimuli, producing high false-positive rate, and will quickly habituate to actual stimuli, leading to high false-negative rate, but will have no trouble maintaining synaptic connections required for stimulus detection. If however intrinsic plasticity is too rigid, the network may find it easier to maintain “optimal” levels of sensitivity, but may have trouble maintaining synaptic connections between stimulus presentations, as low sensitivity would mean low rate of spontaneous replay. The potential solutions to this problem may involve transitioning through distinct developmental stages with different levels of intrinsic plasticity (similar to how we did it in the model, and reminiscent of a known spike of excitability in stage 47 tadpoles [Kara, Ciar]), or distinct learning, maintenance, and operation physiological states regulated by modulatory inputs to the network (a solution that may be related to phases of sleep).

**TODO** *Sensitivity analysis: the sequence in which we destroyed the network.*

# Acknowledgements

My greatest gratitude is to Carlos Aizenman who encouraged me to publish this work as a single author, even though the calcium imaging experiments were performed in his lab, and the materials were paid for by the money from his grant (NSF IOS-1353044). I also thank Heng Xu (Shanghai Jiao Tong University) for his help with first imaging experiments; Joshua Vogelstein (John Hopkins) for his advice on adaptive thresholding; Petko Bogdanov (SUNY Albany), Csilla Szabo (Skidmore College), Gerrit Ansmann (Bonn University), and Jim Belk (St Andrews University) for their help with network science and graph theory, and Sven Anderson (Bard College) for advice on model analysis.

# Citations

Fagiolo, G. (2007). Clustering in complex directed networks. *Physical Review E*, *76*(2), 026107.

Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., & Altman, D. G. (2016). Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations. *European journal of epidemiology*, *31*(4), 337-350.

Kleiner, M., Brainard, D., Pelli, D., Ingling, A., Murray, R., & Broussard, C. (2007). What’s new in Psychtoolbox-3. *Perception*, *36*(14), 1.

Lefèvre, J., & Mangin, J. F. (2010). A reaction-diffusion model of human brain development. *PLoS computational biology*, *6*(4), e1000749.

Leicht, E. A., & Newman, M. E. (2008). Community structure in directed networks. *Physical review letters*, *100*(11), 118703.

Maslov, S., & Sneppen, K. (2002). Specificity and stability in topology of protein networks. *Science*, *296*(5569), 910-913.

Münch, T. A., Da Silveira, R. A., Siegert, S., Viney, T. J., Awatramani, G. B., & Roska, B. (2009). Approach sensitivity in the retina processed by a multifunctional neural circuit. *Nature neuroscience*, *12*(10), 1308.

Pereira, A. G., & Moita, M. A. (2016). Is there anybody out there? Neural circuits of threat detection in vertebrates. *Current opinion in neurobiology*, *41*, 179-187.

Pietri, T., Romano, S. A., Pérez-Schuster, V., Boulanger-Weill, J., Candat, V., & Sumbre, G. (2017). The emergence of the spatial structure of tectal spontaneous activity is independent of visual inputs. *Cell reports*, *19*(5), 939-948.

Rubinov, M., & Sporns, O. (2010). Complex network measures of brain connectivity: uses and interpretations. *Neuroimage*, *52*(3), 1059-1069.

Ruthazer, E. S., & Cline, H. T. (2004). Insights into activity‐dependent map formation from the retinotectal system: A middle‐of‐the‐brain perspective. *Developmental Neurobiology*, *59*(1), 134-146.

Tao, H. W., & Poo, M. M. (2005). Activity-dependent matching of excitatory and inhibitory inputs during refinement of visual receptive fields. *Neuron*, *45*(6), 829-836.

Ng, A., Jordan, M., and Weiss, Y. (2002). On spectral clustering: analysis and an algorithm. In T. Dietterich, S. Becker, and Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems 14 (pp. 849 – 856). MIT Press.

